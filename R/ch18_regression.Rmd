---
title: "Chapter 18 - Regression"
author: "Stefan Diener"
date: "2022-08-17"
output: html_document
---

```{r setup, include=FALSE}
library(glmnet)
```


# Task 3 Regression
### Regression Example

```{r}
dataset <- read.csv('../data/diabetes.csv', header = TRUE)

X <- dataset[,1:10]
y <- dataset[,11, drop=FALSE]

print(dim(X))
print(dim(y))
```

```{r}
# Setting seed for reproducibility
seed = 0

# First split
smp_size <- floor( (1-0.4) * nrow(X) )

set.seed(0)
train_ind <- sample(seq_len(nrow(X)), size = smp_size)

X_train <- X[train_ind, ]
X_rest <- X[-train_ind, ]
y_train <- y[train_ind, , drop=FALSE]
y_rest <- y[-train_ind, , drop=FALSE]

# Second split
smp_size <- floor( (1-0.5) * nrow(X_rest) )

set.seed(0)
train_ind <- sample(seq_len(nrow(X_rest)), size = smp_size)

X_test <- X_rest[train_ind, ]
X_val <- X_rest[-train_ind, ]
y_test <- y_rest[train_ind, , drop=FALSE]
y_val <- y_rest[-train_ind, , drop=FALSE]

print( paste(nrow(y_train), nrow(y_val), nrow(y_test)) )
```

```{r}
# Write custom function to replicate Python approach
sclr_fit_transform <- function(data, scaling_data=data) {
  sclr <- scale(scaling_data)
  output <- scale(data,
                  center = attr(sclr, "scaled:center"),
                  scale = attr(sclr, "scaled:scale")
                  )
  output_df <- data.frame(output)
  colnames(output_df) <- colnames(data)
         
  return(output_df)
}

# Scale to 0 mean and std dev 1 on training data
X_train_scl <- sclr_fit_transform(X_train)
X_val_scl <- sclr_fit_transform(X_val, X_train)
X_test_scl <- sclr_fit_transform(X_test, X_train)
X_scl <- sclr_fit_transform(X, X_train)

print( paste(mean(as.matrix(X_train)), sd(as.matrix(X_train))) )
print( paste(mean(as.matrix(X_train_scl)), sd(as.matrix(X_train_scl))) )
print( paste(mean(as.matrix(X_val)), sd(as.matrix(X_val))) )
print( paste(mean(as.matrix(X_val_scl)), sd(as.matrix(X_val_scl))) )
```


```{r}
# Helper function for preparing dataframes
df_prep <- function(features_df, target_df) {
  output_df <- merge(target_df, features_df, by='row.names', all=TRUE)
  rownames(output_df) <- output_df$Row.names
  output_df$Row.names <- NULL
  return(output_df)
}

# Prepare dataframe for linear model
train_df <- df_prep(X_train_scl, y_train)

# Train a linear model
lr <- lm(target ~ ., data = train_df)

# Define helper functions to calculate the accuracy values on train, validation
# and test set.
mse <- function(X, y, predictor) {
  output <- mean( as.matrix(predict(predictor, X) - y)^2 )
  return(output)
}

rmse <- function(X, y, predictor) {
  return( mse(X,y,predictor)^0.5 )
}

# Print RMSE results
print( paste("Train RMSE:", rmse(X_train_scl, y_train, lr)) )
print( paste("Validation RMSE:", rmse(X_val_scl, y_val, lr)) )
print( paste("Test RMSE:", rmse(X_test_scl, y_test, lr)) )
```



```{r}
# Assign residuals and fitted values to variables
y_train_hat <- lr$fitted.values
res <- lr$residuals

# Print summary statistics
print("Training set:")
print( paste("label mean:", mean(y_train$target), "std:", sd(y_train$target),
              "var:", var(y_train$target)) )
print( paste("residuals mean:", mean(res), "RMSE:", mean(res^2)^.5, "MSE:", mean(res^2)) )
print( paste("R^2:", 1-( mean(res^2)/var(y_train$target) )) )
print("")

# Repeat for validation set
y_val_hat <- predict(lr, X_val_scl)
res_val <- y_val_hat - y_val$target
mse_val <- mean(res_val^2)

print("Validation set:")
print( paste("label mean:", mean(y_val$target), "std:", sd(y_val$target),
             "var:", var(y_val$target)) )
print( paste("residuals mean:", mean(res_val), "RMSE:", mse_val^.5, "MSE:", mse_val) )
print( paste("R^2:", 1-(mse_val/var(y_val$target))) )
```

### Feature Expansion
```{r}
# Degree 1 polynomial feature generation
X_train_sc_pf1 <- model.matrix(target ~ ., data = train_df)

print(colnames(X_train_sc_pf1))
```

```{r}
# Degree 2 polynomial feature generation function
pf2_transform <- function(df) {
  # Write formula for intercept, raw features, squared features, and
  # interactions
  formula_pf2 <- as.formula(paste('target ~ .^2 +',
                                  paste('poly(',
                                        colnames(df)[-c(1,3)],
                                        ',2, raw=TRUE)[, 2]',
                                        collapse = ' + ')
                                  )
                            )
  # We excluded sex^2 in the formula to avoid multicollinearity
  
  output <- model.matrix(formula_pf2, data = df)
  
  # Rewrite column names for readability
  colnames_pf2 <- c("1",
                    colnames(df)[-1], # exclude target
                    paste0(colnames(df)[-1],"^2"), # include squares
                    colnames(output)[-(1:21)]) # include interactions
  colnames(output) <- colnames_pf2
  
  # Convert to dataframe
  output_df <- data.frame(output)
  
  # Also exclude intercept column to avoid rank deficiency issues when using
  # future models
  output_df$X1 <- NULL
  
  return(output_df)
}

# Degree 2 polynomial feature generation
X_train_sc_pf2 <- pf2_transform(train_df)

print(colnames(X_train_sc_pf2))
print(dim(X_train))
print(dim(X_train_sc_pf2))
```



```{r}
# Prepare dataframes for validation and test data
val_df <- df_prep(X_val_scl, y_val)
test_df <- df_prep(X_test_scl, y_test)

# Degree 2 polynomial feature generation for val and test
X_val_sc_pf2 <- pf2_transform(val_df)
X_test_sc_pf2 <- pf2_transform(test_df)

# Train a linear model
# Prepare dataframe for lm function
train_df_pf2 <- df_prep(X_train_sc_pf2, y_train)

lr2 <- lm(target ~ ., data = train_df_pf2)

# Print results
print(paste("Poly2 features, train RMSE:",
            rmse(X_train_sc_pf2, y_train, lr2),
            "val RMSE:",
            rmse(X_val_sc_pf2, y_val, lr2),
            "test RMSE:",
            rmse(X_test_sc_pf2, y_test, lr2))
      )

```

### Ridge Regression
```{r}
# Already excluded the bias column from pf datasets (see Chunk 7)

# Train a linear model
lr_rd1 <- glmnet(as.matrix(X_train_sc_pf2), y_train$target, alpha = 0)
  # alpha = 0 ensures ridge regression

# predict(lr_rd1, as.matrix(X_val_sc_pf2))
print( paste("Ridge train RMSE:",
             rmse(as.matrix(X_train_sc_pf2),y_train,lr_rd1)) )
print( paste("Ridge val   RMSE:",
             rmse(as.matrix(X_val_sc_pf2),y_val,lr_rd1)) )
```

```{r}
# Train a linear model
# Train a linear model
lr_rd1 <- glmnet(as.matrix(X_train_sc_pf2), y_train$target, alpha = 0) # alpha=300?
  # alpha = 0 ensures ridge regression

# predict(lr_rd1, as.matrix(X_val_sc_pf2))
print( paste("Ridge train RMSE:",
             rmse(as.matrix(X_train_sc_pf2),y_train,lr_rd1)) )
print( paste("Ridge val   RMSE:",
             rmse(as.matrix(X_val_sc_pf2),y_val,lr_rd1)) )
```




