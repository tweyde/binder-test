---
title: 'Chapter 19: Interactive Notebook for Students'
author: "Ram Gopal, Dan Philps, and Tillman Weyde"
date: '2022'
output:
  pdf_document:
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
  html_document:
    theme: united
    highlight: tango
    toc: yes
    toc_float: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load libraries
```{r}
library(caret)
library(rattle)
if(!require("pacman")){
  install.packages("pacman")
}
pacman::p_load("remotes", "RnavGraphImageData", "magrittr", "nnet", "data.table",
               "caret", "rpart", "tictoc", "naivebayes", "RSNNS", "mvtnorm", "patchwork")
```

# Get the data
This is an interesting and somewhat larger dataset. This dataset contains grayscale images of faces of 40 people. The each image has 64 x 64 pixels, with values in the range [0,1]. The data depicts 10 images for each person in the dataset. To keep the computations manageable, we will work with data on 5 individuals. This results in 50 rows of data and 4097 columns (1 to label individuals + 64 X 64 pixels). Our machine learning task is to identify individuals based on the grayscale images. Given high dimensionality of the data, you will notice that some of the algorithms take significant compute time.

## Obtain the data from the source
* The following code reads a few libraries and standardizes the data.You can skip to the next section if you want to directly used the saved data file.

```{r}
if(!require("snedata")){
  remotes::install_github("jlmelville/snedata")
}
library(snedata)
library(RnavGraphImageData)
```

```{r}
oli <- as.data.frame(snedata::olivetti_faces())[1:50] 
oli <- as.data.table(snedata::olivetti_faces())[1:50]
y <- oli[, .(Label)] %>%
  .[, Label := factor(Label)]
X <- as.data.table(scale(oli[, !c("Label")]))
y <- oli[, .(Label)] %>%
  .[, Label := factor(Label)]
X <- as.data.table(scale(oli[, !c("Label")]))
oli_sc <- cbind(y, X)
```
## Directly read the data
* You can skip the above steps and directly read the data from the online repository.
```{r}
oli_sc = read.csv("oli_sc.csv")
```
## View the images
* You can view the images by changing the last two parameters below which are the individual and image number for that individual.
```{r}
show_olivetti_face(snedata::olivetti_faces(), 2, 3)
```
# Modeling
## Create Partition

```{r}
set.seed(123456)
oli_sc$Label = as.factor(oli_sc$Label)
index <- createDataPartition(oli_sc$Label,p=0.8,list=FALSE)
train <- oli_sc[index,]
test <- oli_sc[-index,]
trControl <- trainControl(method  = "cv", number  = 2)
```

## Run Models

### Multinomial Regression
* Functions tic and toc from the tictoc package track the compute time for each of the algorithms. Useful to experiment with it across algorithms.
* The model below takes a while to run.
* Note the notation caret::train(). This indicates the function train() from the caret package. This comes in useful as at times the same function name be used by different packages.

```{r include=FALSE, eval=FALSE}
knitr::opts_chunk$set(cache = TRUE)
library(tictoc)
tictoc::tic(msg = "Multinomial Regression 2 fold training time")
log_reg = caret::train(Label ~ .,   method='multinom',
             trControl  = trControl,
             data = train,
             metric = "Accuracy",
              MaxNWts = 1000000,
             maxit = 10)
tictoc::toc()
log_reg_pred_train = predict(log_reg, train)
log_reg_pred_test = predict(log_reg, test)
train_accuracy = caret::confusionMatrix(log_reg_pred_train,train$Label)$overall[1]
test_accuracy = caret::confusionMatrix(log_reg_pred_test,test$Label)$overall[1]
print(paste("Multinomial Regression Accuracy: ",
                "Training = ", train_accuracy,
                " Test = ",test_accuracy))
print(caret::confusionMatrix(log_reg_pred_test,test$Label))
```


### Decision Tree, Naive Bayes, Multi Layer Perceptron algorithms

```{r}
for (mdl in c("rpart","naive_bayes","mlp"))
{
     mdl_model <- caret::train(Label ~ ., 
             method=mdl, 
             trControl  = trControl, 
             data = train, 
             metric     = "Accuracy") 
     mdl_pred_test= predict(mdl_model,test)
     mdl_pred_train= predict(mdl_model,train)
    train_accuracy = caret::confusionMatrix(mdl_pred_train,train$Label)$overall[1]
    test_accuracy = caret::confusionMatrix(mdl_pred_test,test$Label)$overall[1]
    print(paste("Model ", mdl, " Accuracy: ",
                "Training = ", train_accuracy,
                " Test = ",test_accuracy))
    print(caret::confusionMatrix(mdl_pred_test,test$Label))
}
```

## Scaling

```{r}
train1 = as.data.frame(scale(train[,-1]))
test1 = as.data.frame(scale(test[,-1]))
train1$Label = train$Label
test1$Label = test$Label

trControl <- trainControl(method  = "cv", number  = 2)
for (mdl in c("naive_bayes","mlp"))
{
     mdl_model <- caret::train(Label ~ ., 
             method=mdl, 
             trControl  = trControl, 
             data = train1, 
             metric     = "Accuracy") 
     mdl_pred_test= predict(mdl_model,test1)
     mdl_pred_train= predict(mdl_model,train1)
    train_accuracy = caret::confusionMatrix(mdl_pred_train,train1$Label)$overall[1]
    test_accuracy = caret::confusionMatrix(mdl_pred_test,test1$Label)$overall[1]
    print(paste("Model ", mdl, " Accuracy: ",
                "Training = ", train_accuracy,
                " Test = ",test_accuracy))
}
```

# Iris Dataset

```{r}
index <- caret::createDataPartition(iris$Species,p=0.5,list=FALSE) 
train_iris <- iris[index,] 
test_iris <- iris[-index,] 
trControl <- trainControl(method  = "cv", number  = 2)
for (mdl in c("multinom","rpart","naive_bayes","mlp"))
{
     mdl_model <- caret::train(Species ~ ., 
             method=mdl, 
             trControl  = trControl, 
             data = train_iris, 
             metric     = "Accuracy") 
     mdl_pred_test= predict(mdl_model,test_iris)
     mdl_pred_train= predict(mdl_model,train_iris)
    train_accuracy = 
      caret::confusionMatrix(mdl_pred_train,train_iris$Species)$overall[1]
    test_accuracy = 
      caret::confusionMatrix(mdl_pred_test,test_iris$Species)$overall[1]
    print(paste("Model ", mdl, " Accuracy: ",
                "Training = ", train_accuracy,
                " Test = ",test_accuracy))
}
```

# Synthetic Data 1
## Data creation
```{r}
x <- as.data.frame(rmvnorm(n=400, sigma=matrix(c(1,0,0,1), ncol=2))) 
x$class =  factor(ifelse((x$V1 > -1 & x$V1 < 1 )&(x$V2 > -1 & x$V2 < 1 ), 0, 1))
```
## Partition and visualize
```{r}
index <- caret::createDataPartition(x$class,p=0.5,list=FALSE) 
train_x <- x[index,] 
test_x <- x[-index,] 
ggplot(train_x) + 
  geom_point(aes(V1, V2, color = class)) 
```
## Multinomial 
```{r}
trControl <- trainControl(method  = "cv", number  = 2)
log_reg = caret::train(class ~ .,   method='multinom',
             trControl  = trControl,
             data = train_x,
             metric = "Accuracy",
              MaxNWts = 10000000,
             maxit = 10)
log_reg_pred_train = predict(log_reg, train_x)
log_reg_pred_test = predict(log_reg, test_x)
train_accuracy = 
  caret::confusionMatrix(log_reg_pred_train,train_x$class)$overall[1]
test_accuracy = 
  caret::confusionMatrix(log_reg_pred_test,test_x$class)$overall[1]
print(paste("Multinomial Regression Accuracy: ",
                "Training = ", train_accuracy,
                " Test = ",test_accuracy))
ggplot(test_x) + 
  geom_point(aes(V1, V2, color = log_reg_pred_test)) 
```
##Naive Bayes

```{r}
trControl <- trainControl(method  = "cv", number  = 2)
nb_reg = caret::train(class ~ .,   method='naive_bayes',
             trControl  = trControl,
             data = train_x,
             metric = "Accuracy")
nb_reg_pred_train = predict(nb_reg, train_x)
nb_reg_pred_test = predict(nb_reg, test_x)
train_accuracy = 
  caret::confusionMatrix(nb_reg_pred_train,train_x$class)$overall[1]
test_accuracy = 
  caret::confusionMatrix(nb_reg_pred_test,test_x$class)$overall[1]
print(paste("Naive Bayes: ",
                "Training = ", train_accuracy,
                " Test = ",test_accuracy))
ggplot(test_x) + 
  geom_point(aes(V1, V2, color = nb_reg_pred_test)) 
```

## Multi Layer Perceptron

```{r}
trControl <- trainControl(method  = "cv", number  = 2)
mlp_reg = caret::train(class ~ .,   method='mlp',
             trControl  = trControl,
             data = train_x,
             metric = "Accuracy")
mlp_reg_pred_train = predict(mlp_reg, train_x)
mlp_reg_pred_test = predict(mlp_reg, test_x)
train_accuracy = 
  caret::confusionMatrix(mlp_reg_pred_train,train_x$class)$overall[1]
test_accuracy = 
  caret::confusionMatrix(mlp_reg_pred_test,test_x$class)$overall[1]
print(paste("MLP: ",
                "Training = ", train_accuracy,
                " Test = ",test_accuracy))
ggplot(test_x) + 
  geom_point(aes(V1, V2, color = mlp_reg_pred_test)) 
```

# Synthetic Data 2
## Data creation
```{r}
x <- as.data.frame(rmvnorm(n=400, sigma=matrix(c(1,0,0,1), ncol=2))) 
x$class1 = ifelse(x$V1<0,0,1)
x$class2 = ifelse(x$class1==0,1,0)
x$flip = ifelse(runif(nrow(x))<0.3,1,0)
x$class = factor(ifelse(x$flip==1,x$class2,x$class1))
x = x[,c(1,2,6)]
```
## Partition and visualize
```{r}
index <- caret::createDataPartition(x$class,p=0.5,list=FALSE) 
train_x <- x[index,] 
test_x <- x[-index,] 
ggplot(train_x) + 
  geom_point(aes(V1, V2, color = class))
```
## Logistic Regression

```{r}
trControl <- trainControl(method  = "cv", number  = 2)
log_reg = caret::train(class ~ .,   method='multinom',
             trControl  = trControl,
             data = train_x,
             metric = "Accuracy",
              MaxNWts = 10000000,
             maxit = 10)
log_reg_pred_train = predict(log_reg, train_x)
log_reg_pred_test = predict(log_reg, test_x)
train_accuracy = 
  caret::confusionMatrix(log_reg_pred_train,train_x$class)$overall[1]
test_accuracy = 
  caret::confusionMatrix(log_reg_pred_test,test_x$class)$overall[1]
print(paste("Multinomial Regression Accuracy: ",
                "Training = ", train_accuracy,
                " Test = ",test_accuracy))
ggplot(test_x) + 
  geom_point(aes(V1, V2, color = log_reg_pred_test)) 
```

## Multi Layer Perceptron

```{r}
trControl <- trainControl(method  = "cv", number  = 2)
mlp_reg = caret::train(class ~ .,   method='mlp',
             trControl  = trControl,
             data = train_x,
             metric = "Accuracy")
mlp_reg_pred_train = predict(mlp_reg, train_x)
mlp_reg_pred_test = predict(mlp_reg, test_x)
train_accuracy = 
  caret::confusionMatrix(mlp_reg_pred_train,train_x$class)$overall[1]
test_accuracy = 
  caret::confusionMatrix(mlp_reg_pred_test,test_x$class)$overall[1]
print(paste("MLP: ",
                "Training = ", train_accuracy,
                " Test = ",test_accuracy))
ggplot(test_x) + 
  geom_point(aes(V1, V2, color = mlp_reg_pred_test)) 
```

## L2 Regularization

```{r}
trControl <- trainControl(method  = "cv", number  = 2)
mlp_reg = caret::train(class ~ .,   method = 'mlp',
                       learnFuncParams=c(0.01),
             trControl  = trControl,
             data = train_x,
             metric = "Accuracy")
mlp_reg_pred_train = predict(mlp_reg, train_x)
mlp_reg_pred_test = predict(mlp_reg, test_x)
train_accuracy = 
  caret::confusionMatrix(mlp_reg_pred_train,train_x$class)$overall[1]
test_accuracy = 
  caret::confusionMatrix(mlp_reg_pred_test,test_x$class)$overall[1]
print(paste("MLP: ",
                "Training = ", train_accuracy,
                " Test = ",test_accuracy))
ggplot(test_x) + 
  geom_point(aes(V1, V2, color = mlp_reg_pred_test)) 
```


