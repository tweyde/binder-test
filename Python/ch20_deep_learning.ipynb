{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2e788b",
   "metadata": {},
   "source": [
    "# Chapter 20 - Neural Networks and Deep Learning\n",
    "\n",
    "So far, we have only addressed machine learning with classification tasks, i.e. where there is a finite discrete set of target labels, the classes. The other main task in supervised machine learning is regression, where the task is to predict a continuous value. Regression leads to different metrics and algorithms, as well as some new concepts such as loss functions and regularization terms, as it is closer to optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1c889",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "A regression task is one, where the data labels are continuous numbers. In this chapter we will use the diabetes dataset, XXXXXXX "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15725af",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "We are now going the use the iris dataset again, as in the previous chapter. We will, however, do two things differently: 1) we will **scale** the feature values and 2) we will divide the data into 2 subsets, the **training, validation and testing set**.\n",
    "\n",
    "**Scaling:** since we don't make assumptions about the data, it could be that the data values are not suitable  for the machine learning algorithms we apply. E.g. this might cause an error or the optimizer might not converge, so that we don't get a meaningful result. \n",
    "There are different scaling requirements for different models. E.g. decision trees and knn classifiers usually need to scaling. Many other models, like logistic regression or neural networks, as sensitive to scaling.   Standardization, like in chapter 8, to a mean of 0 and a standard deviation of 1 works well for many machine learning. An alternative approach is normalization, which ensures that all values are between 0 and 1. \n",
    "\n",
    "In the code below, we first split the training set from the rest and split the rest into test and validation set. Then we use the Scikit-Learn StandardScaler to standardize the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c15ffae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "dataset = load_diabetes()\n",
    "\n",
    "seed = 0 # this is used with the train_test_split to avoid random behaviour for this demo\n",
    "X = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0433b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265 89 88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[147.22471910112358, 66.47255965222679]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=.4,random_state=seed)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=.5,random_state=seed)\n",
    "print(len(y_train),len(y_val),len(y_test))\n",
    "\n",
    "# most classifiers work better with scaled input data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sclr = StandardScaler()\n",
    "sclr.fit(X_train) # scale to 0 mean and std dev 1 on training data\n",
    "\n",
    "X_train_scl = sclr.fit_transform(X_train) # scale all 3 sets:\n",
    "X_val_scl = sclr.fit_transform(X_val)\n",
    "X_test_scl = sclr.fit_transform(X_test)\n",
    "[np.mean(y_val),np.std(y_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62893f24",
   "metadata": {},
   "source": [
    "### Feed-Forward Neural Network\n",
    "\n",
    "We now use another classifier: the K-Nearest--Neighbor (KNN) classifier. This classifier is very simple: is views the feature values of an item as a vectors. For a new feature vector, we calculate the $k$ closest feature vectors in our training set. We then look up the classes belonging to these $k$ feature vectors and choose the most frequent one as our KNN prediction. The number $k$ determines the behaviour. In the simplest case we can choose $1$, which makes it easy to select the predicted class (no ties).  \n",
    "\n",
    "This is a very simple classifier, but it can be quite effective. In order to select whether to use KNN or Decision Trees, we train both types of model on our training set and calculate the performance on the validation set. We also calculate the performance on the test set, which is a more realistic estimate of the performance on unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8066e80b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j_/122h77c562dczctc6d7kv9mc0000gr/T/ipykernel_25874/297869812.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train both model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MLP regression: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainValTestMse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_scl' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# train both model \n",
    "mlp = MLPRegressor(hidden_layer_sizes=100, random_state=0, max_iter=500)\n",
    "mlp.fit(X_train_scl,y_train)\n",
    "\n",
    "print(\"MLP regression: \",trainValTestMse(mlp))\n",
    "print(\"MLP regr R^2: \",mlp.score(X_train_scl, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc0d6c",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5777efeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b828de",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2288b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce464df4",
   "metadata": {},
   "source": [
    "### Sequence Models to Sequence Models\n",
    "\n",
    "In **k-Fold Cross-Validation**, we divide the whole dataset into $k$ subsets (folds) of approximately equal size. Each fold is used once used as the test set, while the remaining $k-1$ are used as the training set. In this way we get $k$ different samples of the performance. In the extreme case of $k = n$, where $n$ is the size of the dataset, it is called **leave-one-out cross-validation**.\n",
    "\n",
    "When taking a random samples, we will get variation in the distribution of classes between the   ****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bbd0c5",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "cbddda36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold Stratifed CV, test accuracy, mean:  0.7637254901960785 , std dev: 0.05580585675126154\n",
      "Leave-one-out, test accuracy, mean:  0.7696629213483146 , std dev: 0.4210485825292524\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d378fcb8",
   "metadata": {},
   "source": [
    "As we can see, the training set predictions are all correct, as was already clear from the diagram. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70580927",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457925bb",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "074b26097337835a5e1dbb01309c19b1c94bf7c2401f1c92e7358cfe96e9b386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
