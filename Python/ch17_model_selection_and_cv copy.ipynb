{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2e788b",
   "metadata": {},
   "source": [
    "# Chapter 17 - Model Selection and Cross Validation\n",
    "\n",
    "In machine learning, making fewer assummptions and restrictions means that we have many options for choosing and adapting models. One important part of machine learning is therefore to select the models and how to tune them so that we get good generalization.  \n",
    "\n",
    "Model selection is about finding a model that will give good predictions for new data from the same underlying distribution. It can also be about reflecting the process that we are modelling, but that is usually less of a focus in machine learning. The underlying distribution is usually unknown, and without making assumptions we can only use the empirical approach introduced in the previous chapter. \n",
    "\n",
    "There are several ways to select among the vast number of choices for models and for evaluating their performance. Machine learning is often formulated as some form of optimization, i.e. minimising an error metric or maximizing likelyhood. The optimization process can be a closed form clacuation (linear regression), an iterative algorithm (logistic regression), or be a search process (decision tree splits). \n",
    "\n",
    "It is important, however, that the quantity we are ultimately interested in - the prediction quality on unseen data - is by definition not directly available to an optimisation procedure. We therefore need to set up specific approaches to the model optimisation and selection processes to make sure we optimise for the right target. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1c889",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "In machine learning we fit models to training data, but since the goal is to have good predictions for new input data, the performance on the training data is not a good indicator of model quality. Therefore we held out part of the available data as a test dataset. \n",
    "\n",
    "There are sometime cases, we have an effectively infinite supply of labelled data (e.g. text on the internet). However, in most cases, labelled data is not easy to obtain and we need to make the most of the data we have. \n",
    "We will look at two different models and how to determin which one is more suitable for a problem.   \n",
    "\n",
    "### Wine Dataset\n",
    "This time we will look at a different, but also well known, dataset containing information about wine. It contains 13 chemical measurements (features) of different 178 wines made from 3 different grape varieties (classes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c15ffae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "['class_0' 'class_1' 'class_2']\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris, load_digits, load_wine\n",
    "dataset = load_wine()\n",
    "# print(dataset.keys()) # available elements\n",
    "print(dataset['feature_names']) \n",
    "print(dataset['target_names']) # uninformative, funfortunately\n",
    "print(len(dataset.data)) # no of samples\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a0433b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 6\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2,random_state=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62893f24",
   "metadata": {},
   "source": [
    "### Another Classifier - Nearest Neighbor\n",
    "\n",
    "We now use another classifier: the K-Nearest--Neighbor (KNN) classifier. This classifier is very simple: is views the feature values of an item as a vectors. For a new feature vector, we calculate the $k$ closest feature vectors in our training set. We then look up the classes belonging to these $k$ feature vectors and choose the most frequent one as our KNN prediction. The number $k$ determines the behaviour. In the simplest case we can choose $1$, which makes it easy to select the predicted class (no ties). This is a very simple classifier, but it can be quite effective. \n",
    "\n",
    "Our question now is, whether to use KNN or a Decision Tree (DT) classifier. We start by training both models on our training set and calculate the performance on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8066e80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DT:  {'train': 0.9295774647887324, 'test': 0.8333333333333334}\n",
      "KNN:  {'train': 1.0, 'test': 0.7222222222222222}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# train both classifiers\n",
    "dtc = DecisionTreeClassifier(max_depth=2,random_state=0)\n",
    "dtc.fit(X_train,y_train)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# helper functions to calculate the accuracy values on train, validation and test set. \n",
    "def accuracy(X,y,predictor):\n",
    "    return np.sum(np.equal(predictor.predict(X),y))/len(X)\n",
    "\n",
    "def trainValTestAcc(predictor):\n",
    "    vals = {}\n",
    "    vals['train'] = accuracy(X_train,y_train,predictor)\n",
    "    vals['test'] = accuracy(X_test,y_test,predictor)\n",
    "    return vals\n",
    "\n",
    "print(\" DT: \",trainValTestAcc(dtc))\n",
    "print(\"KNN: \",trainValTestAcc(knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc0d6c",
   "metadata": {},
   "source": [
    "We see athat DT seems to work better than KNN on the test set. However, this is just on the 20\\% of the data our test set. Would this be the same, order to test whether data selection is the cause of the difference, we can look at different ways to split the data by changing the value of the `seed` variable above. That has the effecto of changing re results of train_test split. You can try it our for yourself with the notebooks provided with this book, but here are some results to study.\n",
    "\n",
    "`\n",
    "seed = 1\n",
    " DT:  {'train': 0.9295774647887324, 'test': 0.8611111111111112}\n",
    "KNN:  {'train': 1.0, 'test': 0.75}\n",
    "seed = 2\n",
    " DT:  {'train': 0.9295774647887324, 'test': 0.9166666666666666}\n",
    "KNN:  {'train': 1.0, 'test': 0.7222222222222222}\n",
    "seed = 3\n",
    " DT:  {'train': 0.9436619718309859, 'test': 0.7777777777777778}\n",
    "KNN:  {'train': 1.0, 'test': 0.6666666666666666}\n",
    "seed = 4\n",
    " DT:  {'train': 0.9295774647887324, 'test': 0.8888888888888888}\n",
    "KNN:  {'train': 1.0, 'test': 0.8333333333333334}\n",
    "seed = 5\n",
    " DT:  {'train': 0.9366197183098591, 'test': 0.7222222222222222}\n",
    "KNN:  {'train': 1.0, 'test': 0.75}\n",
    "`\n",
    "\n",
    "In most cases, the KNN produces higher accuracy, but in the last one the DT is better. We can also see that there is significant variation between the different splits that we created by using different seeds. The question we need to answer now, is whether the better performance of the DT is due to the underlying process, i.e. the true distribution of model performances on data from underlying distribution, or whether it is likely be the result of random variation. To gain some evidence we need two things: more measurements and a statistical test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce464df4",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "The measurements we are taking are based on randomly selected training and test splits. As we increase the number of splits to test, we will increasligly have overlap between them, because the same datapoints will be reused. To obtain multiple independent test datasets, we would ideally use newly collected data. However, getting more data is often not feasible. A method to make most use of small datasets is called **Cross Validation**. The idea of cross validation is to divide the data systematically such that each data item is used only in one test set. \n",
    "\n",
    "The most common type is **k-Fold Cross-Validation**, where we divide the whole dataset into $k$ subsets (folds) of approximately equal size. Each fold is used once used as the test set, while the remaining $k-1$ are used as the training set. In this way we get $k$ different samples of the performance. Commonly used values for $k$ are between $4$ and $10$. In the extreme case of $k = n$, where $n$ is the size of the dataset, it is called **leave-one-out cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44827d27",
   "metadata": {},
   "source": [
    "There are ready-made implementations of k-fold cross available. \n",
    "\n",
    "[In Scikit-Learn, there KFold class works similar to `train_test_split`, but it generates $k$ splits. `KFold.split()` generates indices that we can use to]: # \n",
    "\n",
    "The function `cross_validate` is available in Scikit-Learn and makes it easy to use cross valiadation. It is left as an exercise to implement your own version of `cross_validate` (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a81869d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold CV, train accuracy, mean:  1.0 , std dev: 0.0\n",
      "10-fold CV, test accuracy, mean:  0.6767973856209151 , std dev: 0.18821061248861198\n",
      "20-fold CV, train accuracy, mean:  1.0 , std dev: 0.0\n",
      "20-fold CV, test accuracy, mean:  0.7180555555555554 , std dev: 0.20568220572788112\n",
      "Leave-one-out, test accuracy, mean:  0.7696629213483146 , std dev: 0.4210485825292524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "cv = KFold(10)\n",
    "scores = cross_validate(knn,X,y,cv=cv,scoring='accuracy',return_train_score=True)\n",
    "print('10-fold CV, train accuracy, mean: ',scores['train_score'].mean(),', std dev:',scores['train_score'].std())\n",
    "print('10-fold CV, test accuracy, mean: ',scores['test_score'].mean(),', std dev:',scores['test_score'].std())\n",
    "\n",
    "cv = KFold(20)\n",
    "scores = cross_validate(knn,X,y,cv=cv,scoring='accuracy',return_train_score=True)\n",
    "print('20-fold CV, train accuracy, mean: ',scores['train_score'].mean(),', std dev:',scores['train_score'].std())\n",
    "print('20-fold CV, test accuracy, mean: ',scores['test_score'].mean(),', std dev:',scores['test_score'].std())\n",
    "\n",
    "cv = LeaveOneOut()\n",
    "scores = cross_validate(knn,X,y,cv=cv,scoring='accuracy')\n",
    "print('Leave-one-out, test accuracy, mean: ',scores['test_score'].mean(),\n",
    "      ', std dev:',scores['test_score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07834c",
   "metadata": {},
   "source": [
    "We can observe that that leave-one-out CV produces higher standard deviation, as to be expected from smaller test sets. When the code is run, we can see that the leave-one-out CV takes longer, because it has to train and test $n = 178$ times, while the k-fold CV only has $k = 10$ iterations.  \n",
    "\n",
    "We also see that the leave-on-out CV produces higher accuracy results. There are two contributing factors here, the larger size of the training set ($n-1$ vs $0.8 n$) and avoiding of class imbalances in the test sets. When taking a random samples, we will get variation in the distribution of classes between the folds. This can skew the accuracy measurements, because the some classes may be harder to predict than others and because the class distribution between the training and test set can be different.  \n",
    "\n",
    "We can ensure that the class distribution in all folds is approximately the same, which is called stratified sampling and leads to **Stratified Cross Validation**. This is implemented in Scikit-Learn as `StratifiedKFold`  and can be used exactly as `KFold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cbddda36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold Stratifed CV, test accuracy, mean:  0.7637254901960785 , std dev: 0.05580585675126154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, LeaveOneOut\n",
    "\n",
    "cv = StratifiedKFold(10,shuffle=True,random_state=0)\n",
    "scores = cross_validate(knn,X,y,cv=cv,scoring='accuracy')\n",
    "print('10-fold Stratifed CV, test accuracy, mean: ',scores['test_score'].mean(),\n",
    "      ', std dev:',scores['test_score'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce142a",
   "metadata": {},
   "source": [
    "We observe that standard deviation of the test accuracy is lower with stratified sampling, as expected because of the more stable classs distribution. We also see an increased accuracy, almost as high as with the leave-one-out CV, which indicates that the effect of the class imbalance was stronger than that of the smaller training sets when comparing k-fold to leave-one-out CV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc7a02",
   "metadata": {},
   "source": [
    "### Significance of Model Differences\n",
    "\n",
    "When trying to assess whether on ML model is better we can use CV (or multiple datasets) to create mulitple independent measurements and test for for significance of their difference. Given that we have no good reasons to assume normality of the distribution of the accuracy values, it is safest to use a non-parametric test. In chapter 9. The Wilcoxon signed-rank test tests for sginificantly different medians. Alternatively, a pairwise t-test could be used, but normality of the distribtion should be tested beforehand. Normality can normally not not be shown, at least for smaller datasets. \n",
    "\n",
    "We will now apply both KNN and DT and apply the Wilcoxong test to the test accuracy results per fold. The p-value gives the probability that a difference between the models of this size or more could have been observed when the underlying distribution had the same median. We normally accept the models as being different when the p-value is below 5%. For significance tests, it is better to have more independent measurements. We can therefore increas the number of folds, as the resulting smaller test sets do not increas the variation between measurements too much, as happened in the leave-one-out CV above. A value of 20 seems like a good guess, as it still leaves ~35 samples in every fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fd9edcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg accuracy KNN: 0.7180555555555554 , DT:  0.8326388888888889\n",
      "avg accuracy KNN std: 0.20568220572788112 , DT:  0.18068771872357806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=22.0, pvalue=0.030066272759898947)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = KFold(20)\n",
    "\n",
    "scores_knn = cross_validate(knn,X,y,cv=cv,scoring='accuracy')\n",
    "scores_dtc = cross_validate(dtc,X,y,cv=cv,scoring='accuracy')\n",
    "print('avg accuracy KNN:',scores_knn['test_score'].mean(),', DT: ',scores_dtc['test_score'].mean())\n",
    "print('avg accuracy KNN std:',scores_knn['test_score'].std(),', DT: ',scores_dtc['test_score'].std())\n",
    "#print(scores_knn['test_score']-scores_dtc['test_score'])\n",
    "\n",
    "from scipy.stats import wilcoxon \n",
    "wilcoxon(scores_dtc['test_score'],scores_knn['test_score'], mode='approx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68bd0a3",
   "metadata": {},
   "source": [
    "The results show that the difference is significant, i.e. the KNN is significantly better than the DT model at predicting Wine type from the chemical measurements. We can also test whether Stratified CV leads to a less variation (stdandard deviation) and better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8bd261af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg accuracy KNN mean: 0.7715277777777778 , DT:  0.8333333333333334\n",
      "avg accuracy KNN std: 0.11304523226141401 , DT:  0.13833221775543036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=21.0, pvalue=0.02473012449507581)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedKFold(20)\n",
    "#cv = StratifiedKFold(10,shuffle=True,random_state=6)\n",
    "\n",
    "scores_knn = cross_validate(knn,X,y,cv=cv,scoring='accuracy')\n",
    "scores_dtc = cross_validate(dtc,X,y,cv=cv,scoring='accuracy')\n",
    "print('avg accuracy KNN mean:',scores_knn['test_score'].mean(),', DT: ',scores_dtc['test_score'].mean())\n",
    "print('avg accuracy KNN std:',scores_knn['test_score'].std(),', DT: ',scores_dtc['test_score'].std())\n",
    "#print(scores_knn['test_score']-scores_dtc['test_score'])\n",
    "\n",
    "from scipy.stats import wilcoxon \n",
    "import scipy\n",
    "wilcoxon(scores_dtc['test_score'],scores_knn['test_score'],mode='approx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d378fcb8",
   "metadata": {},
   "source": [
    "As we can see, all effects of Stratified CV occur: lower standard deviation, higher accuracy and lower p-value, i.e. a more significant result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70580927",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b29eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# # train 3 classifiers\n",
    "# lgr = LogisticRegression()\n",
    "# lgr.fit(X_train_scl, y_train)\n",
    "\n",
    "# dtc = DecisionTreeClassifier(max_depth=2,random_state=0)\n",
    "# dtc.fit(X_train_scl,y_train)\n",
    "\n",
    "# knn = KNeighborsClassifier()\n",
    "# knn.fit(X_train_scl,y_train)\n",
    "\n",
    "# # helper functions to calculate the accuracy values on train, validation and test set. \n",
    "# def accuracy(X,y,predictor):\n",
    "#     return np.sum(np.equal(predictor.predict(X),y))/len(X)\n",
    "\n",
    "# def trainValTestAcc(predictor):\n",
    "#     vals = {}\n",
    "#     vals['train'] = accuracy(X_train_scl,y_train,predictor)\n",
    "#     vals['val'] = accuracy(X_val_scl,y_val,predictor)\n",
    "#     vals['test'] = accuracy(X_test_scl,y_test,predictor)\n",
    "#     return vals\n",
    "\n",
    "# #print(\"logistic regression: \",trainValTestAcc(lgr))\n",
    "# print(\"decision tree: \",trainValTestAcc(dtc))\n",
    "# print(\"KNN: \",trainValTestAcc(knn))\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold, LeaveOneOut\n",
    "\n",
    "# k = 10 # 10-fold cross-validation\n",
    "# #kf = KFold(n_splits=k,shuffle=True,random_state=0)\n",
    "# kf = StratifiedKFold(n_splits=k,shuffle=True,random_state=0)\n",
    "# #kf = RepeatedStratifiedKFold(n_splits=k, n_repeats=10, random_state=0)\n",
    "# #kf = LeaveOneOut()\n",
    "\n",
    "# knn_test_acc = []\n",
    "# for train,test in kf.split(X,y):\n",
    "#     knn.fit(X[train],y[train])\n",
    "#     knn_test_acc.append(np.sum(np.equal(knn.predict(X[test]),y[test]))/len(y[test]))\n",
    "# #print(knn_test_acc)\n",
    "# print(np.mean(knn_test_acc))\n",
    "\n",
    "# dtc_test_acc = []\n",
    "# for train,test in kf.split(X,y):\n",
    "#     dtc.fit(X[train],y[train])\n",
    "#     dtc_test_acc.append(np.sum(np.equal(dtc.predict(X[test]),y[test]))/len(y[test]))   \n",
    "# #print(dtc_test_acc)\n",
    "# print(np.mean(dtc_test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "074b26097337835a5e1dbb01309c19b1c94bf7c2401f1c92e7358cfe96e9b386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
