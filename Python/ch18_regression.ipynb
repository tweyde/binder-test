{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2e788b",
   "metadata": {},
   "source": [
    "# Chapter 18 - Regression Models and Evaluation\n",
    "\n",
    "So far, we have addressed machine learning with classification tasks, i.e. where there is a finite discrete set of target labels, the classes. The other main task in supervised machine learning is regression, where the task is to predict a continuous value. Regression leads to different metrics and algorithms, as well as some new concepts such as loss functions and regularization terms, as it is closer to optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef56186",
   "metadata": {},
   "source": [
    "### Loss Minimization vs Model Estimation\n",
    "\n",
    "Estimation, as introduced in chapter 11, has the goal of finding the best parameter values for a model that represents the data generation process. \n",
    "In Machine Learning, the focus is typically more on the quality of the predictions generated by the model. \n",
    "The quality of the predictions is measured by a loss function, which describes the deviation of the predictions from the actual values we have observed. \n",
    "A lower loss value means a better quality of prediction and the machine learning process should therefore minimize the loss. \n",
    "\n",
    "This can be equivalent to maximising a likelihood of the data under some assumptions and in a regression setting this is often the case.   \n",
    "In a regression setting, where we work with continuous values, we can use also calculus to find the optimial parameter values, which can be very effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62893f24",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "The most common regression model is a linear regression, where we calculate our predicted labels $\\hat{y}$ for a given $D$-dimensional input vector $\\mathbf{x} = (x_1, \\ldots , x_D)$ as a sum weighted with $\\mathbf{w} = (w_1, \\ldots , w_D)$:\n",
    "$$ \\hat{y} = \\sum_{d=1}^D w_d x_d $$\n",
    "or in vector notation:\n",
    "$$ \\hat{y} = \\mathbf{w}^\\top \\mathbf{x}. $$ \n",
    "Often, a intercept or bias $b$ is included $ \\hat{y} = \\sum_d w_d x_d + b $, but that can be integrated into the vectorized solution by adding an element with constant value $1$ to the data vectors $\\mathbf{x} = (x_1, \\ldots , x_D, 1)$ and making $b$ part of $\\mathbf{w} = (w_1, \\ldots , w_D, b)$. \n",
    "We will from now on assume that the bias is included in the data vectors and it can be treated like other data dimensions, unless otherwise indicated. \n",
    "\n",
    "With a dataset of $N$ input/label pairs we can write the dataset as an input matrix $\\mathbf{X}$ of row  \n",
    "$$ \\mathbf{X} = \\left[ \n",
    "\\begin{array}{c} \n",
    "    \\mathbf{x}_1 \\\\ \n",
    "    \\cdots \\\\ \n",
    "    \\mathbf{x}_n\n",
    " \\end{array} \\right] \n",
    " = \\left[ \n",
    " \\begin{array}{c} \n",
    "    x_11, \\ldots , x_1d \\\\ \n",
    "    \\cdots \\\\ \n",
    "    x_n1, \\ldots , x_nd\n",
    " \\end{array} \\right] $$\n",
    "and the labels as a vector $\\mathbf{y} = (y_1, ... , y_n)$. The prediction vector $\\mathbf{\\hat{y}}$ can then be calculated as \n",
    "$$ \\mathbf{\\hat{y}} = \\mathbf{Xw}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b0de7",
   "metadata": {},
   "source": [
    "### Least Squares Optimization\n",
    "\n",
    "The most common loss function for regression functions is the **sum of squared errors** (SSE). It is calculated as \n",
    "$$ SSE = \\sum_{k=1}^d (y_k - \\hat{y}_k)^2 = (\\mathbf{y} - \\mathbf{\\hat{y}})^\\top (\\mathbf{y} - \\mathbf{\\hat{y}}) = (\\mathbf{y-Xw})^\\top(\\mathbf{y-Xw})$$\n",
    "\n",
    "In the case of SSE with a linar model, there is a closed-form solution. \n",
    "As a sketch, it can be found as follows. \n",
    "We need the gradient of SSE with respoct to $\\mathbf{w}$:  \n",
    "$$ \\nabla_w SSE = 2 (\\mathbf{X}^\\top \\mathbf{X w} âˆ’ \\mathbf{X}^\\top \\mathbf{y}) $$ \n",
    "Seeting the gradient to 0 we can work out the optimal $\\mathbf{\\hat{w}}$:\n",
    "\n",
    "$$ \\mathbf{X}^\\top \\mathbf{X \\hat{w}} = \\mathbf{X}^\\top \\mathbf{y} $$\n",
    "\n",
    "From there we can find the solution:\n",
    "$$ \\mathbf{\\hat{w}} = ( \\mathbf{X}^\\top \\mathbf{X \\hat{w}} )^{-1} \\mathbf{X}^\\top \\mathbf{y} $$\n",
    "\n",
    "A more rigorous mathematical treatment can be found, e.g., in James et al (2013): *An introduction to statistical learning*, New York: Springer.\n",
    "\n",
    "In practice, we often take the average SE per sample, the **mean squared error** (MSE):\n",
    "$$ MSE = \\frac{SSE}{N} = \\frac{1}{N} \\sum_{k=1}^N (y_n - \\hat{y}_n)^2. $$\n",
    "The MSE has the advantage over the SSE that it is comparable between datssets of differentsize. \n",
    "You may have noticed that the MES is the population variance if the mean of the residuals is 0. In other words, the variance of the data is the MSE of a predictor that predicts the mean of the labels. \n",
    "\n",
    "Another frequently used used error metric is the **root mean squared error** (RMSE):\n",
    "$$ RMSE = \\sqrt{\\frac{SSE}{N}} = \\sqrt{\\frac{1}{N} \\sum_{k=1}^N (y_n - \\hat{y}_n)^2}. $$\n",
    "The RMSE metric is on the same scale as the label dtaa, so that we can interpret the RMSE in units of the measured quantity, e.g., physical or finacial units. \n",
    "Since the standard deviation is the square root of the variance, the RMSE is the population based standard deviation of the residuals if they have a mean of 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a84b6c",
   "metadata": {},
   "source": [
    "### Text Box: Least Squares as Maximum Likelihood\n",
    "\n",
    "Minimizing SSE fits with the following model formulation for normally distributed target values: \n",
    "$$ p(y|\\mathbf{x}) = \\mathit{N}(y-\\mathbf{w}^T\\mathbf{x},\\sigma^2) $$ with $\\mathbf{x}$ and $\\mathbf{w}$ as above. \n",
    "\n",
    "The total log-likelihood is \n",
    "\n",
    "$$ LL(\\mathbf{w}, \\sigma^2) = \\sum_{n=1}^N \\log \\mathit{N}(\\mathbf{y}_n-\\mathbf{w}^T\\mathbf{x}_n,\\sigma^2) = \\sum_{n=1}^N \\log \\mathit{N}(\\mathbf{y}_n-\\mathbf{\\hat{y}}_n,\\sigma^2) $$\n",
    "\n",
    "Written into the definition this can be worked out as: \n",
    "\n",
    "$$ LL(x) = \\sum_{n=1}^N \\log \\frac{1}{\\sqrt{\\sigma^22\\pi}}exp\\left(-\\frac{(\\mathbf{y}_n-\\mathbf{\\hat{y}}_n)^2}{2\\sigma^2)}\\right) $$\n",
    "\n",
    "$$ = -\\frac{1}{2\\sigma^2} \\sum_{n=1}^N (\\mathbf{y}_n-\\mathbf{\\hat{y}}_n)^2 + \\frac{N}{2} \\log (2 \\pi \\sigma^2) $$\n",
    " \n",
    "In a full model estimation, we would optimize for $\\mathbf{w}$ and $\\sigma^2$. \n",
    "However, for producing predictions it is sufficient to just optimize for $\\mathbf{w}$. \n",
    "For this purpose the tems relating to $\\sigma^2$ can be see as constants that don't influence the optimization, so that the term to maxmize looks like this: \n",
    "$$ -c_1 \\sum_{n=1}^N (\\mathbf{y}_n-\\mathbf{\\hat{y}}_n)^2 + c_2, $$\n",
    "where only relevant term is the negative SSE.\n",
    "Therefore, minimizing the SSE maximises the LL if we assume that the errors are normally distributed.\n",
    "\n",
    "This assumption may often not be true in practice, the estimates of of $\\mathbf{w}$ (or $\\sigma^2$) or not valid as probabilistic model parameters. \n",
    "\n",
    "If the main goal is to optimise prediction quality, minimizing SSE is still useful. \n",
    "\n",
    "For a more detailed propabilistic treatment of machine learning see \n",
    "Murphy, K (2022). *Probabilistic machine learning: an introduction*. MIT press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1c889",
   "metadata": {},
   "source": [
    "## Regression Example\n",
    "\n",
    "In this chapter we will use the diabetes dataset, this dataset has 10 attributes about a patient as features and disease progression after a year as the target. The dataset contains data for 442 patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b411f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10) (442,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "dataset = load_diabetes()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9c01f",
   "metadata": {},
   "source": [
    "Generally, cross-validation is the appropriate method for this dataset, but for simplicity we will operate with single training, validation, and test sets for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9768047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265 89 88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 0 # this is used with the train_test_split to avoid random behaviour for this demo\n",
    "\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=.4,random_state=seed)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=.5,random_state=seed)\n",
    "print(len(y_train),len(y_val),len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815fef0",
   "metadata": {},
   "source": [
    "In this chapter, we will use mostly numeric optimization and most numeric optimization works better on scaled data. \n",
    "We used standardization, where we ensure that the training data has a mean of 0 and a standard deviation of 1. \n",
    "We transform the validation and test data, too, but we don't fit the scaling to them, as that would not be possible with new data that would not be available at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "301343fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00031869540828427685, 0.04878425077127095]\n",
      "[9.384526698718305e-18, 1.0000000000000002]\n",
      "[-0.000980652215373006, 0.046564689060377244]\n",
      "[-3.991813122247754e-18, 0.9999999999999999]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sclr = StandardScaler()\n",
    "sclr.fit(X_train) # scale to 0 mean and std dev 1 on training data\n",
    "\n",
    "X_train_scl = sclr.fit_transform(X_train) # scale all 3 sets:\n",
    "X_val_scl = sclr.fit_transform(X_val)\n",
    "X_test_scl = sclr.fit_transform(X_test)\n",
    "print([np.mean(X_train),np.std(X_train)])\n",
    "print([np.mean(X_train_scl),np.std(X_train_scl)])\n",
    "print([np.mean(X_val),np.std(X_val)])\n",
    "print([np.mean(X_val_scl),np.std(X_val_scl)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc71801",
   "metadata": {},
   "source": [
    "With the available programming libraries in Python or R, linear regression is very easy to use, just like the  models we used previously for classification, they have `fit` and `predict` methods, so that the code is very similar to the previous chapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8066e80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression RMSE results:  {'train': 51.78004290628544, 'val': 55.175429959614796, 'test': 59.00716319829052}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# train a linar model \n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scl,y_train)\n",
    "\n",
    "# helper functions to calculate the accuracy values on train, validation and test set. \n",
    "def rmse(X,y,predictor):\n",
    "    return (mse(X,y,predictor))**.5\n",
    "#    return np.sqrt(mse(X,y,predictor))\n",
    "\n",
    "\n",
    "# helper functions to calculate the accuracy values on train, validation and test set. \n",
    "def mse(X,y,predictor):\n",
    "    return ((predictor.predict(X)-y)**2).mean()\n",
    "\n",
    "\n",
    "def trainValTestMse(predictor):\n",
    "    vals = {}\n",
    "    vals['train'] = rmse(X_train_scl,y_train,predictor)\n",
    "    vals['val'] = rmse(X_val_scl,y_val,predictor)\n",
    "    vals['test'] = rmse(X_test_scl,y_test,predictor)\n",
    "    return vals\n",
    "\n",
    "print(\"Linear regression RMSE results: \",trainValTestMse(lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1baa53e",
   "metadata": {},
   "source": [
    "The numbers can be put into context by looking at the distribution of the target data, the predictions, and the residuals and calculating the $R^2$ (coefficient of determination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "40994001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "label mean     :  152.3811320754717 , std:  79.81375740069201 , var:  6370.235870416519\n",
      "residuals  mean:  1.7160277391942042e-15 , RMSE:  51.78004290628544 , MSE:  2681.172843376761\n",
      "R^2:  0.5791093300283947\n",
      "Validation Set\n",
      "label mean:  147.22471910112358 , std:  66.47255965222679 , var:  4418.601186718849\n",
      "residuals   mean:  5.156412974348101 , RMSE:  55.175429959614796 , MSE:  3044.328071228358\n",
      "R^2:  0.31101994894248297\n"
     ]
    }
   ],
   "source": [
    "y_train_hat = lr.predict(X_train_scl) # prediction vector\n",
    "resid = y_train_hat-y_train # residuals vector\n",
    "print(\"Training Set\")\n",
    "print('label mean     : ', y_train.mean(),', std: ', y_train.std(),', var: ', y_train.var()) \n",
    "print('residuals  mean: ', resid.mean(),', RMSE: ',((resid**2).mean())**.5,', MSE: ', (resid**2).mean() )\n",
    "print('R^2: ',1-((resid**2).mean()/np.var(y_train)))\n",
    "\n",
    "y_val_hat = lr.predict(X_val_scl) # prediction vector\n",
    "resid_val = y_val_hat-y_val # residuals vector\n",
    "mse_val = (resid_val**2).mean()\n",
    "print(\"Validation Set\")\n",
    "print('label mean: ', np.mean(y_val),', std: ',np.std(y_val),', var: ', np.var(y_val)) \n",
    "print('residuals   mean: ',resid_val.mean(),', RMSE: ',mse_val**.5,', MSE: ', mse_val) \n",
    "print('R^2: ',1-(mse_val/np.var(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d8b9e",
   "metadata": {},
   "source": [
    "We can see that the average of the resiuals is very close to 0 on the training data, so our model has (almost) no bias for over- or under-predicting.\n",
    "The R<sup>2</sup> on the training set is 57.8%, on the validation set  it is 31.1%. I.e., the error is reduced compared to a model predicting always the mean, but there is clearly room for improvement. \n",
    "Similarly, the RMSE on is lower than than the standard deviation of the label data, but only by 35% and 17%, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b971742",
   "metadata": {},
   "source": [
    "### Feature Expansion to Address Underfitting\n",
    "\n",
    "A lacking precision of the predictions is called **underfitting**, i.e. the model does not adapt well to the data. \n",
    "This is typically caused by the the model not being flexible enough, if the data is consistent (i.e. there are not two different labels for the same input vector). \n",
    "\n",
    "One way to increase the flexiblity of the modle is by processing the data, so that we have more features that relate to the input in different ways. \n",
    "One method to achieve this is to create polynomial features that include products of input features. \n",
    "E.g., for two inputs $x_1,x_2$, the polynomial features up to a maximal degree of 2 would be $1,x_1,x_2,x_1^2,x_1x_2,x_2^2$. \n",
    "In other words, we are adding interaction terms (products of two different variables)\n",
    "     and quadratic terms to the regression eqation, so that the number of coefficients (i.e. the weights $w_1, ... ,w_m$) increases. \n",
    "For higher degrees $max_d$, the features will contain all possible polynomials of degrees up to $max_d$. \n",
    "<mark>perhaps discuss # of poly features</mark> \n",
    "\n",
    "This is implemented in sklearn in the class `PolynomialFeatures`, which also offers a method `get_feature_names_out`, that describes the generated features. \n",
    "Using `degree = 1`, it just generates one additional feature of degree 0, i.e. a constant feature $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "35f2cd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' 'x0' 'x1' 'x2' 'x3' 'x4' 'x5' 'x6' 'x7' 'x8' 'x9']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pf1 = PolynomialFeatures(degree=1)\n",
    "X_train_sc_pf1 = pf1.fit_transform(X_train_scl)\n",
    "print(pf1.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0da02f",
   "metadata": {},
   "source": [
    "By increasing degree = 2, we add all possible polynomials of degree 2 so that we get 66 features in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4013f52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' 'x0' 'x1' 'x2' 'x3' 'x4' 'x5' 'x6' 'x7' 'x8' 'x9' 'x0^2' 'x0 x1'\n",
      " 'x0 x2' 'x0 x3' 'x0 x4' 'x0 x5' 'x0 x6' 'x0 x7' 'x0 x8' 'x0 x9' 'x1^2'\n",
      " 'x1 x2' 'x1 x3' 'x1 x4' 'x1 x5' 'x1 x6' 'x1 x7' 'x1 x8' 'x1 x9' 'x2^2'\n",
      " 'x2 x3' 'x2 x4' 'x2 x5' 'x2 x6' 'x2 x7' 'x2 x8' 'x2 x9' 'x3^2' 'x3 x4'\n",
      " 'x3 x5' 'x3 x6' 'x3 x7' 'x3 x8' 'x3 x9' 'x4^2' 'x4 x5' 'x4 x6' 'x4 x7'\n",
      " 'x4 x8' 'x4 x9' 'x5^2' 'x5 x6' 'x5 x7' 'x5 x8' 'x5 x9' 'x6^2' 'x6 x7'\n",
      " 'x6 x8' 'x6 x9' 'x7^2' 'x7 x8' 'x7 x9' 'x8^2' 'x8 x9' 'x9^2']\n",
      "(265, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(265, 66)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf2 = PolynomialFeatures(degree=2)\n",
    "X_train_sc_pf2 = pf2.fit_transform(X_train_scl)\n",
    "print(pf2.get_feature_names_out())\n",
    "print(X_train.shape)\n",
    "X_train_sc_pf2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f04191",
   "metadata": {},
   "source": [
    "When can now train a model with the transformed dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f0a97aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poly2 Features, train RMSE:  44.509225398475564 , val RMSE:  9008763272046.398\n"
     ]
    }
   ],
   "source": [
    "X_val_sc_pf2 = pf2.transform(X_val_scl)\n",
    "X_test_sc_pf2 = pf2.transform(X_test_scl)\n",
    "\n",
    "# train a linar model \n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(X_train_sc_pf2,y_train)\n",
    "\n",
    "print('Poly2 Features, train RMSE: ',rmse(X_train_sc_pf2,y_train,lr2),\n",
    "      ', val RMSE: ' ,rmse(X_val_sc_pf2,y_val,lr2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bddcde",
   "metadata": {},
   "source": [
    "The results show a much reduced error on the training set, but a astronomically large error on the validation set. \n",
    "If you inspect the coefficients, you'll see what is happening (see exercise below). \n",
    "This is an extreme case of **overfitting**\n",
    "\n",
    "Too much flexibility of the model can apparently lead to problems, too, specifically to overfitting. \n",
    "This can be understood by thinking of a very flexible model that can perfectly reproduce the training data. \n",
    "Apart from the fact that 'memorising' the data is not too useful, polynomials of high degrees also have the property that they produce steep slopes and thus a large variation of values between datapoints. \n",
    "\n",
    "<mark>We can illustrate this with a simple polynomial on a 1 d dataset.</mark>\n",
    "\n",
    "Another problem is that with a higher number of variables, there is a higher chance for numerical problems in the optimization. \n",
    "Especially, if there are more features than datapoints the system is underdeterminned and there is no unique solution. The implementation in `LinearRegression` is numerically very robust and can deal with underdetermined systems, but thatis not guaranteed in general. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7970cb60",
   "metadata": {},
   "source": [
    "### $L_2$ and $L_1$ Regularization\n",
    "\n",
    "Both the overfitting and potential numerical problems can be addressed by regularization. \n",
    "The general concept of regularization as a way to reduce model flexibility, as introduced in the previous chapters, has very popular implementations for linear regression and related models, including general linear models (chapter 13) and neural networks. \n",
    "Because these models use numeric optimization methods, we can introduce additive terms in the loss function that relate to the model, that will be minimised together with the prediction loss. \n",
    "The most common such loss terms are $L_1$ and $L_2$ norms of the weights. \n",
    "\n",
    "The term norm describes ways of calculating the length of the vector. \n",
    "The standard norm is the Euclidian norm or $L_2$ norm, which can also be written as $|| \\mathbf{w} ||_2$, which is defined for a $D$-dimensional vector $\\mathbf{w}$ as $\\sqrt{\\sum_{d=1}^D w_d^2}$ and describes the length in physical space for $d=3$. \n",
    "The $L_1$ norm is defined as $\\sum_{d=1}^D \\text{abs}(w_d)$, where $\\text{abs}$ sets the sign of the argument to positive. The $L_1$ norm can be interpreted as the distance on a rectangular grid and is also called the *Manhattan norm*.  \n",
    "\n",
    "The $L_2$ norm regularizing loss $l_r$, also called Tikhonov regularization, is defined as follows: \n",
    "$$loss_2 = \\sum_d (\\hat{y}_d - y)^2 + \\sum_d (w_d)^2 = || \\mathbf{Xw} - \\mathbf{y} ||_2^2 + \\alpha || \\mathbf{w} ||_2^2 $$ \n",
    "and the $L_1$ loss as \n",
    "$$loss_1 = \\sum_d (\\hat{y}_d - y)^2 + \\sum_d \\text{abs}(w_d) = || \\mathbf{Xw} - \\mathbf{y} ||_2^2 + \\alpha || \\mathbf{w} ||_1 $$. \n",
    "\n",
    "In both cases, the idea is that the optimizer reduces the size of the model coefficients, so that weights that don't contribute only little to reducing the predictin error will be kept small. \n",
    "\n",
    "For simple linear regression, there are specific names: $L_2$- and $L_1$-regularised regressions are called ridge and lasso regression, and in both cases, the term shrinkage is also used (as the regularisation shrinks the coefficients). $L_2$-regularization is often very effective as is penalizes larger weight values more. $L_1$-regularization has the positive property of leading to sparse coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31883e4",
   "metadata": {},
   "source": [
    "### Parameter Shrinkage with Ridge Regression\n",
    "\n",
    "Let us start by applying a $L_2$ or  idge regression, which is again implemented as a standard predictor class `Ridge` in Scikit-Learn. One practical aspect is important here, which is to not have a intercept (or bias) term in the data, but to have it calculated by the ridge model (default in Scikit-Learn), because we don't want the intercept to be regularized.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e3a4a3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge train RMSE:  45.0006645588884\n",
      "Ridge val   RMSE:  61.94445869321387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "pf2n = PolynomialFeatures(include_bias=False)\n",
    "X_train_sc_pf2 = pf2n.fit_transform(X_train_scl)\n",
    "X_val_sc_pf2 = pf2n.fit_transform(X_val_scl)\n",
    "X_test_sc_pf2 = pf2n.fit_transform(X_test_scl)\n",
    "\n",
    "# train a linar model \n",
    "lr_rd1 = Ridge(fit_intercept=True)\n",
    "lr_rd1.fit(X_train_sc_pf2,y_train)\n",
    "\n",
    "print('Ridge train RMSE: ',rmse(X_train_sc_pf2,y_train,lr_rd1))\n",
    "print('Ridge val   RMSE: ',rmse(X_val_sc_pf2,y_val,lr_rd1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934dd336",
   "metadata": {},
   "source": [
    "We can see that the error on the training data has gone up slightly and on the validation data, it has gone down greatly. \n",
    "We still have room to improve, so let's try stonger regularization with a higher $\\alpha$ value. \n",
    "This is a hyper-parameter to tune, like in the previous chapter. \n",
    "We could use the cross-validation plot, but for brevitiy we'll just use a value that was found by a grid search. \n",
    "Implementing a grid search on the train/validation split and a CV grid search are exericse (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d9748a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.23942319481683\n",
      "51.64815860679413\n"
     ]
    }
   ],
   "source": [
    "lr_rd2 = Ridge(alpha=300,fit_intercept=True)\n",
    "lr_rd2.fit(X_train_sc_pf2,y_train)\n",
    "\n",
    "print(rmse(X_train_sc_pf2,y_train,lr_rd2))\n",
    "print(rmse(X_val_sc_pf2,y_val,lr_rd2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d821f6a",
   "metadata": {},
   "source": [
    "We see that with a suitable $\\alpha$ value, we can indeed reduce overfitting and obtain a better model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c6169199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "label mean    :  152.3811320754717 , std:  79.81375740069201 , var:  6370.235870416519\n",
      "residuals mean:  1.1154180304762328e-14 , RMSE:  53.23942319481683 , MSE:  2834.4361821168004\n",
      "R^2:  0.555050042137377\n",
      "Validation Set\n",
      "label mean    :  147.22471910112358 , RMSE:  66.47255965222679 , MSE:  4418.601186718849\n",
      "residuals mean:  6.794860872917203 , RMSE:  51.64815860679413 , MSE:  2667.5322874725625\n",
      "R^2:  0.39629485107403184\n"
     ]
    }
   ],
   "source": [
    "y_train_hat = lr_rd2.predict(X_train_sc_pf2) # prediction vector\n",
    "resid = y_train_hat-y_train # residuals vector\n",
    "print(\"Training Set\")\n",
    "print('label mean    : ',np.mean(y_train),', std: ',np.std(y_train),', var: ', np.var(y_train)) \n",
    "print('residuals mean: ',np.mean(resid),', RMSE: ',np.std(resid),', MSE: ', np.var(resid)) \n",
    "print('R^2: ',1-(np.var(resid)/np.var(y_train)))\n",
    "\n",
    "y_val_hat = lr_rd2.predict(X_val_sc_pf2) # prediction vector\n",
    "resid_val = y_val_hat - y_val # residuals vector\n",
    "print(\"Validation Set\")\n",
    "print('label mean    : ',np.mean(y_val),', RMSE: ',np.std(y_val),', MSE: ', np.var(y_val)) \n",
    "print('residuals mean: ',np.mean(resid_val),', RMSE: ',(resid_val**2).mean()**.5,\n",
    "      ', MSE: ', (resid_val**2).mean()) \n",
    "print('R^2: ',1-((resid_val**2).mean()/np.var(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f593956",
   "metadata": {},
   "source": [
    "### Feature Selection with Lasso Regression\n",
    "\n",
    "As mentioned, $L_1$-regression can lead to sparse coefficients. That means, it drives weights equally towards 0 even if they are very small (as opposed to $L_2$ where the regularisation term gets quadratically smaller for smaller coefficient values). This leads to many coefficients actually reaching 0 after fitting them model, so that they have no influence on the predictions. \n",
    "\n",
    "This allows us to understand easily which features are most important for predicting the target. Also, it meanse that don't need to gather values for these 0-weighted features in future applications of the model. This can save time, money and computation in actually applications.\n",
    "\n",
    "Let's try this on the diabetes dataset. Again there is a convenient class in Scikit-Learn, `Lasso` that allows us to creat and fit a model with minimal code changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6ec7d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso train RMSE):  47.37809431050939\n",
      "Lasso val RMSE):    55.61032458728394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# train a linar model \n",
    "lr_lso = Lasso()\n",
    "lr_lso.fit(X_train_sc_pf2,y_train)\n",
    "\n",
    "print('Lasso train RMSE): ',rmse(X_train_sc_pf2,y_train,lr_lso))\n",
    "print('Lasso val RMSE):   ',rmse(X_val_sc_pf2,y_val,lr_lso))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e402c2",
   "metadata": {},
   "source": [
    "Although there is clearly less overfitting than with ridge regression, we can still try to reduce is further. With a little testing, we can find that 10 is a good value for the $\\alpha$ parameter. Determining this value systematically is again left as an exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "da9bbe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso alpha=300 train RMSE):  54.393944437527104\n",
      "Lasso alpha=300 val RMSE):    52.807267316335164\n"
     ]
    }
   ],
   "source": [
    "# train a linar model \n",
    "lr_lso2 = Lasso(alpha=10)\n",
    "lr_lso2.fit(X_train_sc_pf2,y_train)\n",
    "\n",
    "print('Lasso alpha=300 train RMSE): ',rmse(X_train_sc_pf2,y_train,lr_lso2))\n",
    "print('Lasso alpha=300 val RMSE):   ',rmse(X_val_sc_pf2,y_val,lr_lso2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0738e",
   "metadata": {},
   "source": [
    "This error value is slightly higher than with the ridge regression, as can also be seen in the $R^2$ value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dbfbb3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "label mean     :  152.3811320754717 , std:  79.81375740069201 , var:  6370.235870416519\n",
      "residuals  mean:  1.201219417435943e-14 , RMSE:  54.3939444375271 , MSE:  2958.701191472785\n",
      "R^2:  0.5355429136913058\n",
      "Validation Set\n",
      "label mean      :  147.22471910112358 , RMSE:  66.47255965222679 , MSE:  4418.601186718849\n",
      "residuals   mean:  5.768981549451163 , RMSE:  52.807267316335164 , MSE:  2788.60748141888\n",
      "R^2:  0.37642565670267825\n"
     ]
    }
   ],
   "source": [
    "y_train_hat = lr_lso2.predict(X_train_sc_pf2) # prediction vector\n",
    "resid = y_train_hat-y_train # residuals vector\n",
    "print(\"Training Set\")\n",
    "print('label mean     : ',np.mean(y_train),', std: ',np.std(y_train),', var: ', np.var(y_train)) \n",
    "print('residuals  mean: ',np.mean(resid),', RMSE: ',np.std(resid),', MSE: ', np.var(resid)) \n",
    "print('R^2: ',1-(np.var(resid)/np.var(y_train)))\n",
    "\n",
    "y_val_hat = lr_lso2.predict(X_val_sc_pf2) # prediction vector\n",
    "print(\"Validation Set\")\n",
    "print('label mean      : ',np.mean(y_val),', RMSE: ',np.std(y_val),', MSE: ', np.var(y_val)) \n",
    "print('residuals   mean: ',np.mean(y_val_hat-y_val),', RMSE: ',rmse(X_val_sc_pf2,y_val,lr_lso2),\n",
    "      ', MSE: ', mse(X_val_sc_pf2,y_val,lr_lso2)) \n",
    "print('R^2: ',1-(np.var(y_val_hat-y_val)/np.var(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5a5e8",
   "metadata": {},
   "source": [
    "However, the differences are small and may be spcific to the validation set. \n",
    "For more reliable results, cross-validation should be applied and a significance test. \n",
    "Even if the Lasso model is slightly worse, depending on the application it may be more valuable to be able to get predictions with fewer mesures. E.g. it may allow you to take mesurements more often. \n",
    "\n",
    "We can extract the coefficients and see which features have non-zero cofficients. \n",
    "In the case of the diabetes data, we can seee that only 6 of the original features (X0,X1,X2,X5,X7,X9) are actually used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "46ad30a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso coefficients [ 0.         -0.         21.09885135  7.25809404 -0.         -0.\n",
      " -4.66809785  0.         23.92567557  0.          0.          2.284511\n",
      "  0.          0.         -0.         -0.         -0.          0.\n",
      "  0.          0.         -0.          0.          0.          0.\n",
      " -0.          0.         -0.          0.          0.          2.11981431\n",
      "  0.         -0.         -0.         -0.          0.          0.\n",
      "  0.          2.1011182   0.         -0.          0.         -0.\n",
      "  0.          0.         -0.         -0.          0.         -0.\n",
      " -0.          0.         -0.          0.         -0.         -0.\n",
      "  0.         -0.          0.          0.         -0.         -0.\n",
      " -0.          0.          0.          0.          0.        ]\n",
      "Positions of non-zero coefficients (array([ 2,  3,  6,  8, 11, 29, 37]),)\n",
      "Names and values of non-zero coefficients [('x1', 21.098851350283923), ('x2', 7.258094039818483), ('x5', -4.668097852769056), ('x7', 23.925675570857106), ('x0^2', 2.2845109952089255), ('x1 x9', 2.1198143136713385), ('x2 x9', 2.1011182026125352)]\n"
     ]
    }
   ],
   "source": [
    "print('Lasso coefficients',lr_lso2.coef_)\n",
    "print('Positions of non-zero coefficients',np.nonzero(lr_lso2.coef_))\n",
    "nz_feature_names = pf2.get_feature_names_out()[np.nonzero(lr_lso2.coef_)]\n",
    "nz_feature_values = lr_lso2.coef_[np.nonzero(lr_lso2.coef_)]\n",
    "print('Names and values of non-zero coefficients',list(zip(nz_feature_names,nz_feature_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db43bc0",
   "metadata": {},
   "source": [
    "### Optimizating Non-linear Models  \n",
    "\n",
    "Above we have used a polynomial extension of the model on the input side, by extracting interaction and quadratic combinations. \n",
    "When the parameters in these feature extractors are fixed, the model is still linear with respect to the feature values. \n",
    "However, using non-linear functions on the input side with learnable parameters is can make models much more flexibly while retaining control over the number of parameters in the model. \n",
    "This approach is used, e.g., in neural networks. \n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "For non-linear models, there is in general no closed-form solution available to fit the model parameters to the data. In the general case (so called non-convex optimisation problems) is not general algorithm for finding the optimal solution or even for knowing fi we found it. The most common idea for optimising non-linear models to use a technique called gradient descent. For this we calculate the gradient of our loss function with respect to the parameters. The gradient tells us whether the error will increase if we change the parameters in a particular direction by an arbitrarily small amount. By moving in the opostite direction, we hope to obtain a reduction in the loss. As the gradient changes size and direction as we change the paramters, we make only small changes as repeat the process until we find a gradient approaches zero and we cannot improve the error. In so-called convex problems, which include linear regression, this will lead us to an optimal solution. However, most linear models are not convex, so that we cannot be sure that change will lead us to the best solution. The main problem is that we may find a local minimum, which may be far from optimal. \n",
    "Despite these problems, for complex problems and large datasets, non-linear models tend to perform better. \n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "The most commonly used continuous non-linear model are probably neural networks. We will start with a simple neural network for regression. It is like a linear regression with $k$ features, where each is the a linear combination of the input data followed by non-linear activation function $f$:\n",
    "$$ \\hat{y} = \\sum_k{ w2_{\\_k} f( \\sum_d w_{k\\_d} x_1} ), $$\n",
    "where $\\mathbf{w1}$ is the weight vector the output, $\\hat{y}, and \\mathbf{w2} is the matrix of wheights for the $k$ features. In neural netowrks, the output and feature values are seen as the activation of connected neurons. These neurons were originally meant as modles of biological neurons, but today they are generally used as building blocks for computational learning systems.  The neurons are organised in layers. In our case, there is an input layer, the input data, one hidden layer, the $k$ features, and the output layer, containing only our ouptut value. \n",
    "\n",
    "<mark>add a NN diagram here</mark>\n",
    "\n",
    "This striuctre is called a multi-layer perceptron (MLP). I has been proven mathematically, that with enough hidden neurons, there exist a weight configuration such that the model approximats any function on a compact interval with arbitrary precision. That does not, however, mean that our learning algorithms find that weight configuration for a given set of datapoints. \n",
    "\n",
    "In practical terms, we can use an MLP similarly to a linear regression, with two main differences: the MLP can learn non-linear functions but in order to achieve good results, we need to find good setting for the many hyper-parameters of an MLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ef89c6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP train RMSE):  45.43298375574876\n",
      "MLP val RMSE):    85.262901745132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tweyde/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(random_state=0,hidden_layer_sizes=5)\n",
    "mlp.fit(X_train_sc_pf2,y_train/100)\n",
    "y_hat = mlp.predict(X_train_sc_pf2)\n",
    "\n",
    "print('MLP train RMSE): ',rmse(X_train_sc_pf2,y_train/100,mlp)*100)\n",
    "print('MLP val RMSE):   ',rmse(X_val_sc_pf2,y_val/100,mlp)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc0d6c",
   "metadata": {},
   "source": [
    "### Regression Trees\n",
    "\n",
    "Another approach to regression is to not use a weighted sums at all, but to divide the data up according to feature thresholds. This approach is used in regression trees, which operate very much like decision trees, with exception that they use the variance of the labels in a node. I.e., they split the nodes such that the average variance in the split nodes is as low as possible. For inference, a new input sample is processed until it reaches a leaf and then the average label value of that leaf is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c393f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT regression:  {'train': 57.111047461837266, 'val': 58.72111971799094, 'test': 60.250876375949275}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "# train both model \n",
    "dtr = DecisionTreeRegressor(max_depth=2)\n",
    "dtr.fit(X_train_scl,y_train)\n",
    "\n",
    "print(\"DT regression: \",trainValTestMse(dtr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70580927",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18785ff",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Inspect the cofficients of the linear model trained on the polynomial features. Find the coefficient responsible for the large error and explain why it has a especially strong effect on the polynomial features. \n",
    "2. Implement a grid search for the alpha parameter of a linear regression on the train/validation split with the poly 2 features of the diabetes data. \n",
    "3. Implement a grid search for the alpha parameter of a linear regression using cross validation with the poly 2 features of the diabetes data. Use the Scikit-Learn class `GridSearchCV`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08163738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(265, 3003)\n",
      "1.7646137495089635e-12 3.113861684956083e-24\n",
      "313.1626828357963 98070.86592091357\n"
     ]
    }
   ],
   "source": [
    "# Poly 3 Features\n",
    "pf3 = PolynomialFeatures(degree=5)\n",
    "X_train_sc_pf3 = pf3.fit_transform(X_train_scl)\n",
    "X_val_sc_pf3 = pf3.transform(X_val_scl)\n",
    "\n",
    "print(X_train_sc_pf3.shape)\n",
    "\n",
    "# train a linar model \n",
    "lr3 = LinearRegression()\n",
    "lr3.fit(X_train_sc_pf3,y_train)\n",
    "\n",
    "print(rmse(X_train_sc_pf3,y_train,lr3),mse(X_train_sc_pf3,y_train,lr3))\n",
    "print(rmse(X_val_sc_pf3,y_val,lr3),mse(X_val_sc_pf3,y_val,lr3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "074b26097337835a5e1dbb01309c19b1c94bf7c2401f1c92e7358cfe96e9b386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
