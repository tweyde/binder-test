{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19: Classification Models and Evaluation\n",
    "## Exercises solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Implement your own version of the classification metrics precision, recall and F1 score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import  the modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives and measures the ability of a classifier of not labeling as positive a sample that is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(true, pred, pos_lab = 1):\n",
    "    \n",
    "    '''\n",
    "    Function:\n",
    "            calculates the precision score \n",
    "    \n",
    "    Inputs:\n",
    "            true labels (list), predictions (list), positive_label (int, str)\n",
    "    \n",
    "    Output:\n",
    "            precision score (float)\n",
    "    '''\n",
    "\n",
    "    if len(true) != len(pred):\n",
    "        raise ValueError (f'True and predicted labels should have the same length. Received sizes ({len(true)}, {len(pred)}).')\n",
    "    tp = 0 # true positives\n",
    "    fp = 0 # false positives\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == pred[i]:\n",
    "            tp += 1\n",
    "        if true[i] != pos_lab and pred[i] == pos_lab:\n",
    "            fp += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives and measures the ability of the classifier to identify all the positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(true, pred, pos_lab = 1):\n",
    "    \n",
    "    '''\n",
    "    Function:\n",
    "            calculates the recall score \n",
    "    \n",
    "    Inputs:\n",
    "            true labels (list), predictions (list), positive_label (int, str)\n",
    "    \n",
    "    Output:\n",
    "            recall score (float)\n",
    "    '''\n",
    "\n",
    "    if len(true) != len(pred):\n",
    "        raise ValueError (f'True and predicted labels should have the same length. Received sizes ({len(true)}, {len(pred)}).')\n",
    "    tp = 0 # true positives\n",
    "    fn = 0 # negative labels\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == pred[i]:\n",
    "            tp += 1\n",
    "        if true[i] == pos_lab and pred[i] != pos_lab:\n",
    "            fn += 1\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "The F1 score is the harmonic mean of precision and recall: 2 * (precision * recall) / (precision + recall) hence measures the model performance as a function of precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(true, pred, pos_lab = 1):\n",
    "    \n",
    "    '''\n",
    "    Function:\n",
    "            calculates the F1 score \n",
    "    \n",
    "    Inputs:\n",
    "            true labels (list), predictions (list), positive_label (int, str)\n",
    "    \n",
    "    Output:\n",
    "            F1 score (float)\n",
    "    '''\n",
    "    \n",
    "    if len(true) != len(pred):\n",
    "        raise ValueError (f'True and predicted labels should have the same length. Received sizes ({len(true)}, {len(pred)}).')\n",
    "    tp = 0 # true positives\n",
    "    fp = 0 # false positives\n",
    "    fn = 0 # false negatives\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == pred[i]:\n",
    "            tp += 1\n",
    "        if true[i] != pos_lab and pred[i] == pos_lab:\n",
    "            fp += 1\n",
    "        if true[i] == pos_lab and pred[i] != pos_lab:\n",
    "            fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Load the *digits* dataset and test the classifiers introduced in this chapter on it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules\n",
    "import sklearn\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# load the dataframe \n",
    "df = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset type: \n",
      "<class 'sklearn.utils.Bunch'>\n",
      "\n",
      "Dataset attributes: \n",
      "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n",
      "\n",
      "Feaures names: \n",
      "['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7']\n",
      "\n",
      "Target: \n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "Number of samples: \n",
      "1797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# store the features in a variable X\n",
    "X = df.data\n",
    "# store the targets in a variable y \n",
    "y = df.target\n",
    "\n",
    "# we can see that this dataset is a class \n",
    "print(f'Dataset type: \\n{type(df)}\\n')\n",
    "# show the dataset keys\n",
    "print(f'Dataset attributes: \\n{df.keys()}\\n')\n",
    "# these are our features (X)\n",
    "print(f'Feaures names: \\n{df.feature_names}\\n')\n",
    "# these are our targets (y)\n",
    "print(f'Target: \\n{df.target_names}\\n')\n",
    "# number of samples\n",
    "print(f'Number of samples: \\n{len(df.data)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# show an image from the dataset\n",
    "plt.gray()\n",
    "plt.matshow(df.images[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "We start creating the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the training set: 1257\n",
      "Number of targets in the training set: 1257\n",
      "Number of features in the test set: 540\n",
      "Number of targtes in the test set: 540\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# set the seed to promote the reproducibility of the results (it controls the randomness in the splits in this case)\n",
    "seed = 0\n",
    "\n",
    "# split features and targets in train and test sets (with stratification).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = seed, stratify = y)\n",
    "\n",
    "print(f'Number of features in the training set: {len(X_train)}')\n",
    "print(f'Number of targets in the training set: {len(y_train)}')\n",
    "print(f'Number of features in the test set: {len(X_test)}')\n",
    "print(f'Number of targtes in the test set: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalise the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scl mean: -1.589818173687575e-18, X_train_scl std: 0.9682458365518543\n",
      "X_test_scl mean: 0.002949341824474758, X_test_scl std: 1.1151159288845494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# scale the features\n",
    "sclr = StandardScaler() \n",
    "X_train_scl = sclr.fit_transform(X_train) # scale to 0 mean and std dev 1 on training data \n",
    "X_test_scl = sclr.transform(X_test) \n",
    "\n",
    "print(f'X_train_scl mean: {np.mean(X_train_scl)}, X_train_scl std: {np.std(X_train_scl)}')\n",
    "print(f'X_test_scl mean: {np.mean(X_test_scl)}, X_test_scl std: {np.std(X_test_scl)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier() \n",
      "Training mean accuracy: 1.0000 - Test mean accuracy: 0.8519 - Time to train: 0.0227\n",
      "\n",
      "GaussianNB() \n",
      "Training mean accuracy: 0.8210 - Test mean accuracy: 0.8111 - Time to train: 0.0021\n",
      "\n",
      "LogisticRegression(max_iter=2000, random_state=0) \n",
      "Training mean accuracy: 0.9984 - Test mean accuracy: 0.9741 - Time to train: 0.0694\n",
      "\n",
      "MLPClassifier(max_iter=2000, random_state=0) \n",
      "Training mean accuracy: 1.0000 - Test mean accuracy: 0.9759 - Time to train: 0.6858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.neural_network import MLPClassifier  \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "import time\n",
    "\n",
    "# build the models \n",
    "dtc = DecisionTreeClassifier() \n",
    "gnb = GaussianNB() \n",
    "lr = LogisticRegression(max_iter=2000,random_state=0) \n",
    "mlp = MLPClassifier(max_iter=2000,random_state=0) \n",
    "\n",
    "# loop through the models, train and test them\n",
    "for mdl in [dtc, gnb, lr,  mlp]:\n",
    "    start = time.time() \n",
    "    mdl.fit(X_train_scl,y_train) \n",
    "    t_time = time.time()- start\n",
    "    print(mdl, f'\\nTraining mean accuracy: {mdl.score(X_train_scl,y_train):.4f} - Test mean accuracy: {mdl.score(X_test_scl,y_test):.4f} - Time to train: {(t_time):.4f}\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that different models not only show different levels of adaptation and generalization, but need also different times for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Implement parameter tuning on the digits datasets for the logistic regression and the MLP classifier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.040887</td>\n",
       "      <td>0.008008</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.7</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.7000000000000001, 'penalty': 'l2'}</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.97619</td>\n",
       "      <td>0.952191</td>\n",
       "      <td>0.964143</td>\n",
       "      <td>0.972112</td>\n",
       "      <td>0.966578</td>\n",
       "      <td>0.008231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.036629</td>\n",
       "      <td>0.006812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.5, 'penalty': 'l2'}</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.97619</td>\n",
       "      <td>0.952191</td>\n",
       "      <td>0.960159</td>\n",
       "      <td>0.972112</td>\n",
       "      <td>0.966575</td>\n",
       "      <td>0.008978</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.035941</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.6</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.6, 'penalty': 'l2'}</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.97619</td>\n",
       "      <td>0.952191</td>\n",
       "      <td>0.960159</td>\n",
       "      <td>0.972112</td>\n",
       "      <td>0.966575</td>\n",
       "      <td>0.008978</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "25       0.040887      0.008008         0.000201        0.000402     0.7   \n",
       "17       0.036629      0.006812         0.000000        0.000000     0.5   \n",
       "21       0.035941      0.001554         0.000200        0.000400     0.6   \n",
       "\n",
       "   param_penalty                                      params  \\\n",
       "25            l2  {'C': 0.7000000000000001, 'penalty': 'l2'}   \n",
       "17            l2                 {'C': 0.5, 'penalty': 'l2'}   \n",
       "21            l2                 {'C': 0.6, 'penalty': 'l2'}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "25           0.968254            0.97619           0.952191   \n",
       "17           0.972222            0.97619           0.952191   \n",
       "21           0.972222            0.97619           0.952191   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "25           0.964143           0.972112         0.966578        0.008231   \n",
       "17           0.960159           0.972112         0.966575        0.008978   \n",
       "21           0.960159           0.972112         0.966575        0.008978   \n",
       "\n",
       "    rank_test_score  \n",
       "25                1  \n",
       "17                2  \n",
       "21                2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# soppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set the parameters to search\n",
    "lr_parameters = {'penalty': ['l1', 'l2', 'none', 'elasticnet'],\n",
    "                 'C': np.arange(0.1, 1, 0.1)}\n",
    "\n",
    "# set a 5 fold nested grid search and use test the logistic regression classifier previously built\n",
    "clf = GridSearchCV(lr, lr_parameters, cv = 5)\n",
    "clf.fit(X_train_scl, y_train)\n",
    "\n",
    "# display the top ranked models \n",
    "pd.DataFrame(clf.cv_results_).sort_values(by = 'rank_test_score').head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Logistic Regression\n",
    "We re-train the logistic regression classifier with the best hyperparameters and test it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mean accuracy: 0.9741\n"
     ]
    }
   ],
   "source": [
    "lr_tuned = LogisticRegression(C = 0.7, penalty = 'l2', max_iter=2000,random_state=0) \n",
    "lr_tuned.fit(X_train_scl, y_train)\n",
    "print(f'Test mean accuracy: {lr_tuned.score(X_test_scl,y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the tuning, we are unable to improve the logistic regression mean accuracy on the test set. That was already a very high score though! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the MLP\n",
    "**Note**: The below cell may take a few minutes to complete. <br>We searched a relatively large hyperparemter space (36 combinations) and the MLP is a computationally expensive model to run (as seen above). If you wish to speed up your tuning, remove some parameters from the below *mlp_param* dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.805281</td>\n",
       "      <td>0.065060</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>relu</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>2000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'hidden_layer_sizes': (...</td>\n",
       "      <td>0.97619</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.980080</td>\n",
       "      <td>0.976096</td>\n",
       "      <td>0.964143</td>\n",
       "      <td>0.976921</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.837741</td>\n",
       "      <td>0.054502</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>relu</td>\n",
       "      <td>50</td>\n",
       "      <td>2000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'hidden_layer_sizes': 5...</td>\n",
       "      <td>0.97619</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.960159</td>\n",
       "      <td>0.960159</td>\n",
       "      <td>0.972112</td>\n",
       "      <td>0.971343</td>\n",
       "      <td>0.010534</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.196593</td>\n",
       "      <td>0.069572</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>relu</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>2000</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'hidden_layer_sizes': (...</td>\n",
       "      <td>0.97619</td>\n",
       "      <td>0.980159</td>\n",
       "      <td>0.964143</td>\n",
       "      <td>0.968127</td>\n",
       "      <td>0.964143</td>\n",
       "      <td>0.970553</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "32       0.805281      0.065060         0.000401        0.000491   \n",
       "29       0.837741      0.054502         0.000386        0.000473   \n",
       "31       2.196593      0.069572         0.000200        0.000400   \n",
       "\n",
       "   param_activation param_hidden_layer_sizes param_max_iter param_solver  \\\n",
       "32             relu                   (100,)           2000         adam   \n",
       "29             relu                       50           2000         adam   \n",
       "31             relu                   (100,)           2000          sgd   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "32  {'activation': 'relu', 'hidden_layer_sizes': (...            0.97619   \n",
       "29  {'activation': 'relu', 'hidden_layer_sizes': 5...            0.97619   \n",
       "31  {'activation': 'relu', 'hidden_layer_sizes': (...            0.97619   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "32           0.988095           0.980080           0.976096   \n",
       "29           0.988095           0.960159           0.960159   \n",
       "31           0.980159           0.964143           0.968127   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "32           0.964143         0.976921        0.007737                1  \n",
       "29           0.972112         0.971343        0.010534                2  \n",
       "31           0.964143         0.970553        0.006513                3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP - we can specify its architecture by defining the number of neurons at each layer \n",
    "mlp_param = {'hidden_layer_sizes': [(50), (100,), (50, 50)],\n",
    "             'activation': ['identity','logistic', 'tanh', 'relu'], # Activation function for the hidden layer\n",
    "             'solver': ['lbfgs', 'sgd', 'adam'], # The solver for weight optimization\n",
    "             'max_iter': [2000]}\n",
    "\n",
    "# set a 5 fold nested grid search and use test the logistic regressor previously built\n",
    "clf = GridSearchCV(mlp, mlp_param, cv = 5)\n",
    "clf.fit(X_train_scl, y_train)\n",
    "\n",
    "# display the top ranked models \n",
    "pd.DataFrame(clf.cv_results_).sort_values(by = 'rank_test_score').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'max_iter': 2000,\n",
       " 'solver': 'adam'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the best hyperparameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above hyperparameters are the same as the default ones (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html), meaning that we could not find a better model than the one we already implemented. However, we achieved a mean accuracy of 0.9759 on the test set, which is really high!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Load the *Wisconsin breast cancer dataset* and predict the tumor type. Use different models, tune their hyper-parameters, select the best model and estimate its performance on new data. Justify the choices or methods and models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "df = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset type: \n",
      "<class 'sklearn.utils.Bunch'>\n",
      "\n",
      "Dataset attributes: \n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "\n",
      "Feaures names: \n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "\n",
      "Target: \n",
      "['malignant' 'benign']\n",
      "\n",
      "Number of samples: \n",
      "569\n"
     ]
    }
   ],
   "source": [
    "# store the features in a variable X\n",
    "X = df.data\n",
    "# store the targets in a variable y \n",
    "y = df.target\n",
    "\n",
    "# we can see that this dataset is a class \n",
    "print(f'Dataset type: \\n{type(df)}\\n')\n",
    "# show the dataset keys\n",
    "print(f'Dataset attributes: \\n{df.keys()}\\n')\n",
    "# these are our features (X)\n",
    "print(f'Feaures names: \\n{df.feature_names}\\n')\n",
    "# these are our targets (y)\n",
    "print(f'Target: \\n{df.target_names}\\n')\n",
    "# number of samples\n",
    "print(f'Number of samples: \\n{len(df.data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "We start creating the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the training set: 455\n",
      "Number of targets in the training set: 455\n",
      "Number of features in the test set: 114\n",
      "Number of targtes in the test set: 114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# set the seed to promote the reproducibility of the results (it controls the randomness in the splits in this case)\n",
    "seed = 0\n",
    "\n",
    "# split features and targets in train and test sets (with stratification).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = seed, stratify = y)\n",
    "\n",
    "print(f'Number of features in the training set: {len(X_train)}')\n",
    "print(f'Number of targets in the training set: {len(y_train)}')\n",
    "print(f'Number of features in the test set: {len(X_test)}')\n",
    "print(f'Number of targtes in the test set: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalise the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scl mean: 1.0827317878249146e-16, X_train_scl std: 0.9999999999999999\n",
      "X_test_scl mean: 0.01146158680974278, X_test_scl std: 1.0398570731281607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "# scale the features\n",
    "sclr = StandardScaler() \n",
    "X_train_scl = sclr.fit_transform(X_train) # scale to 0 mean and std dev 1 on training data \n",
    "X_test_scl = sclr.transform(X_test) \n",
    "\n",
    "print(f'X_train_scl mean: {np.mean(X_train_scl)}, X_train_scl std: {np.std(X_train_scl)}')\n",
    "print(f'X_test_scl mean: {np.mean(X_test_scl)}, X_test_scl std: {np.std(X_test_scl)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier() \n",
      "Training mean accuracy: 1.0000 - Test mean accuracy: 0.9035 - Time to train: 0.0042\n",
      "\n",
      "GaussianNB() \n",
      "Training mean accuracy: 0.9363 - Test mean accuracy: 0.9035 - Time to train: 0.0010\n",
      "\n",
      "LogisticRegression(max_iter=2000, random_state=0) \n",
      "Training mean accuracy: 0.9890 - Test mean accuracy: 0.9825 - Time to train: 0.0070\n",
      "\n",
      "MLPClassifier(max_iter=2000, random_state=0) \n",
      "Training mean accuracy: 1.0000 - Test mean accuracy: 0.9561 - Time to train: 0.4959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.neural_network import MLPClassifier  \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "import time\n",
    "\n",
    "# build the models \n",
    "dtc = DecisionTreeClassifier() \n",
    "gnb = GaussianNB() \n",
    "lr = LogisticRegression(max_iter=2000,random_state=0) \n",
    "mlp = MLPClassifier(max_iter=2000,random_state=0) \n",
    "\n",
    "# loop through the models, train and test them\n",
    "for mdl in [dtc, gnb, lr,  mlp]:\n",
    "    start = time.time() \n",
    "    mdl.fit(X_train_scl,y_train) \n",
    "    t_time = time.time()- start\n",
    "    print(mdl, f'\\nTraining mean accuracy: {mdl.score(X_train_scl,y_train):.4f} - Test mean accuracy: {mdl.score(X_test_scl,y_test):.4f} - Time to train: {(t_time):.4f}\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier():\n",
      "{'max_depth': 13, 'min_samples_leaf': 5, 'min_samples_split': 4}\n",
      "\n",
      "GaussianNB():\n",
      "{'var_smoothing': 1e-09}\n",
      "\n",
      "LogisticRegression(max_iter=2000, random_state=0):\n",
      "{'C': 0.2, 'penalty': 'l2'}\n",
      "\n",
      "MLPClassifier(max_iter=2000, random_state=0):\n",
      "{'activation': 'logistic', 'hidden_layer_sizes': 50, 'max_iter': 2000, 'solver': 'adam'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# soppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set the hyperparameters to search for the decision tree\n",
    "dtc_param = {'max_depth':np.arange(1, 20, 4), \n",
    "                  'min_samples_split':np.arange(2, 8, 2),\n",
    "                  'min_samples_leaf':np.arange(1, 20, 4)}\n",
    "\n",
    "# set the hyperparameters to search for the gaussian naive bayes\n",
    "gnb_param = {'var_smoothing': [1e-09, 1e-08, 1e-07]}\n",
    "\n",
    "# set the hyperparameters to search for the logistic regression\n",
    "lr_param = {'penalty': ['l1', 'l2', 'none', 'elasticnet'],\n",
    "                 'C': np.arange(0.1, 1, 0.1)}\n",
    "\n",
    "# set the hyperparameters to search for the multi layer perceptron\n",
    "mlp_param = {'hidden_layer_sizes': [(50), (100,)],\n",
    "             'activation': ['identity','logistic', 'tanh', 'relu'], # Activation function for the hidden layer\n",
    "             'solver': ['lbfgs', 'sgd', 'adam'], # The solver for weight optimization\n",
    "             'max_iter': [2000]}\n",
    "\n",
    "# store all the hyperparameters here\n",
    "hyperparameters = [dtc_param, gnb_param, lr_param, mlp_param]\n",
    "# store all the models here\n",
    "models = [dtc, gnb, lr,  mlp]\n",
    "\n",
    "# loop through the models and their hyperparameters and print the best hyperparameters\n",
    "for i in range(len(models)):\n",
    "    clf = GridSearchCV(models[i], hyperparameters[i], cv = 5)\n",
    "    clf.fit(X_train_scl, y_train)\n",
    "    print(f'{models[i]}:\\n{clf.best_params_}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-train the models with the best hyperparameters and test them on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=13, min_samples_leaf=5, min_samples_split=4) \n",
      "Test mean accuracy: 0.9298 - Time to train: 0.0050\n",
      "\n",
      "GaussianNB() \n",
      "Test mean accuracy: 0.9035 - Time to train: 0.0010\n",
      "\n",
      "LogisticRegression(C=0.2, max_iter=2000, random_state=0) \n",
      "Test mean accuracy: 0.9649 - Time to train: 0.0054\n",
      "\n",
      "MLPClassifier(activation='logistic', hidden_layer_sizes=50, max_iter=2000,\n",
      "              random_state=0) \n",
      "Test mean accuracy: 0.9737 - Time to train: 0.2315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the models \n",
    "dtc = DecisionTreeClassifier(max_depth = 13, min_samples_leaf = 5, min_samples_split = 4) \n",
    "gnb = GaussianNB(var_smoothing = 1e-09) \n",
    "lr = LogisticRegression(C = 0.2, penalty = 'l2', max_iter=2000,random_state=0) \n",
    "mlp = MLPClassifier(activation = 'logistic', hidden_layer_sizes = 50, max_iter = 2000, solver = 'adam', random_state=0) \n",
    "\n",
    "# loop through the models, train and test them\n",
    "for mdl in [dtc, gnb, lr,  mlp]:\n",
    "    start = time.time() \n",
    "    mdl.fit(X_train_scl,y_train) \n",
    "    t_time = time.time()- start\n",
    "    print(mdl, f'\\nTest mean accuracy: {mdl.score(X_test_scl,y_test):.4f} - Time to train: {(t_time):.4f}\\n') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree and the MLP improved their accuracy on the test set (*generalization*) after hyperparameters tuning. It is especially interesting to note how the MLP classifier achieved a better performance with a smaller architecture (50 hidden neurons instead of 100) and a different activation function (logistic vs relu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Load the Covertype dataset and predict the classes. Model this multi-class problem with different models and determine their optimal hyper-parameters, select the best model and estimate its performance on new data. Justify the choices or methods and models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get (Error -3) when importing this dataset go to \"/Users/{home_folder}/scikit_learn_data/cover_type\" and delete the samples_py3 file. \n",
    "from sklearn.datasets import fetch_covtype\n",
    "df = fetch_covtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset attributes: \n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])\n",
      "\n",
      "Feaures names: \n",
      "['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_0', 'Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3', 'Soil_Type_0', 'Soil_Type_1', 'Soil_Type_2', 'Soil_Type_3', 'Soil_Type_4', 'Soil_Type_5', 'Soil_Type_6', 'Soil_Type_7', 'Soil_Type_8', 'Soil_Type_9', 'Soil_Type_10', 'Soil_Type_11', 'Soil_Type_12', 'Soil_Type_13', 'Soil_Type_14', 'Soil_Type_15', 'Soil_Type_16', 'Soil_Type_17', 'Soil_Type_18', 'Soil_Type_19', 'Soil_Type_20', 'Soil_Type_21', 'Soil_Type_22', 'Soil_Type_23', 'Soil_Type_24', 'Soil_Type_25', 'Soil_Type_26', 'Soil_Type_27', 'Soil_Type_28', 'Soil_Type_29', 'Soil_Type_30', 'Soil_Type_31', 'Soil_Type_32', 'Soil_Type_33', 'Soil_Type_34', 'Soil_Type_35', 'Soil_Type_36', 'Soil_Type_37', 'Soil_Type_38', 'Soil_Type_39']\n",
      "\n",
      "Target: \n",
      "['Cover_Type']\n",
      "\n",
      "Classes: \n",
      "{1, 2, 3, 4, 5, 6, 7}\n",
      "\n",
      "Number of samples: \n",
      "581012\n"
     ]
    }
   ],
   "source": [
    "# store the features in a variable X\n",
    "X = df.data\n",
    "# store the targets in a variable y \n",
    "y = df.target\n",
    "\n",
    "# show the dataset keys\n",
    "print(f'Dataset attributes: \\n{df.keys()}\\n')\n",
    "# these are our features (X)\n",
    "print(f'Feaures names: \\n{df.feature_names}\\n')\n",
    "# these are our targets (y)\n",
    "print(f'Target: \\n{df.target_names}\\n')\n",
    "# print the classes names \n",
    "print(f'Classes: \\n{set(df.target)}\\n')\n",
    "# number of samples\n",
    "print(f'Number of samples: \\n{len(df.data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start creating the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the training set: 406708\n",
      "Number of targets in the training set: 406708\n",
      "Number of features in the test set: 174304\n",
      "Number of targtes in the test set: 174304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# set the seed to promote the reproducibility of the results (it controls the randomness in the splits in this case)\n",
    "seed = 0\n",
    "\n",
    "# split features and targets in train and test sets (with stratification).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = seed, stratify = y)\n",
    "\n",
    "print(f'Number of features in the training set: {len(X_train)}')\n",
    "print(f'Number of targets in the training set: {len(y_train)}')\n",
    "print(f'Number of features in the test set: {len(X_test)}')\n",
    "print(f'Number of targtes in the test set: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalise the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scl mean: 6.284881784727393e-18, X_train_scl std: 1.0000000000002802\n",
      "X_test_scl mean: -4.70230698938807e-05, X_test_scl std: 0.9999097325165021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "# scale the features\n",
    "sclr = StandardScaler() \n",
    "X_train_scl = sclr.fit_transform(X_train) # scale to 0 mean and std dev 1 on training data \n",
    "X_test_scl = sclr.transform(X_test) \n",
    "\n",
    "print(f'X_train_scl mean: {np.mean(X_train_scl)}, X_train_scl std: {np.std(X_train_scl)}')\n",
    "print(f'X_test_scl mean: {np.mean(X_test_scl)}, X_test_scl std: {np.std(X_test_scl)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier() \n",
      "Training mean accuracy: 1.0000 - Test mean accuracy: 0.9354 - Time to train: 4.3820\n",
      "\n",
      "GaussianNB() \n",
      "Training mean accuracy: 0.0881 - Test mean accuracy: 0.0880 - Time to train: 0.2361\n",
      "\n",
      "LogisticRegression(max_iter=2000, random_state=0) \n",
      "Training mean accuracy: 0.7244 - Test mean accuracy: 0.7248 - Time to train: 137.6145\n",
      "\n",
      "MLPClassifier(max_iter=2000, random_state=0) \n",
      "Training mean accuracy: 0.8773 - Test mean accuracy: 0.8733 - Time to train: 386.5008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.neural_network import MLPClassifier  \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "import time\n",
    "\n",
    "# build the models \n",
    "dtc = DecisionTreeClassifier() \n",
    "gnb = GaussianNB() \n",
    "lr = LogisticRegression(max_iter=2000,random_state=0) \n",
    "mlp = MLPClassifier(max_iter=2000,random_state=0) \n",
    "\n",
    "# loop through the models, train and test them\n",
    "for mdl in [dtc, gnb, lr, mlp]:\n",
    "    start = time.time() \n",
    "    mdl.fit(X_train_scl,y_train) \n",
    "    t_time = time.time()- start\n",
    "    print(mdl, f'\\nTraining mean accuracy: {mdl.score(X_train_scl,y_train):.4f} - Test mean accuracy: {mdl.score(X_test_scl,y_test):.4f} - Time to train: {(t_time):.4f}\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is much larger than the previous ones and the difference in training times between the models becomes more substantial. This is something especially important to keep in mind in model selection and when we run the gridsearch for hyperparameters tuning. Since the decision tree was the best performer and very quick to train, in this case we will focus just on tuning its hyperparameters. The MLP took about 93 times longer to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning\n",
    "**Note:** The cell below may take 10 minutes or lunger to run. <br>\n",
    "We fine tune the decision tree that is the best model in terms of performance and training times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3.166164</td>\n",
       "      <td>0.139585</td>\n",
       "      <td>0.020091</td>\n",
       "      <td>0.005735</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>{'max_depth': 16, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.859753</td>\n",
       "      <td>0.863576</td>\n",
       "      <td>0.860023</td>\n",
       "      <td>0.863193</td>\n",
       "      <td>0.858608</td>\n",
       "      <td>0.861031</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2.813059</td>\n",
       "      <td>0.123110</td>\n",
       "      <td>0.012917</td>\n",
       "      <td>0.008818</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>{'max_depth': 16, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.858634</td>\n",
       "      <td>0.862580</td>\n",
       "      <td>0.858585</td>\n",
       "      <td>0.862554</td>\n",
       "      <td>0.857784</td>\n",
       "      <td>0.860027</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2.746378</td>\n",
       "      <td>0.032168</td>\n",
       "      <td>0.018482</td>\n",
       "      <td>0.007021</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 16, 'min_samples_leaf': 1, 'min_...</td>\n",
       "      <td>0.857650</td>\n",
       "      <td>0.861154</td>\n",
       "      <td>0.857589</td>\n",
       "      <td>0.861189</td>\n",
       "      <td>0.856272</td>\n",
       "      <td>0.858771</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "49       3.166164      0.139585         0.020091        0.005735   \n",
       "50       2.813059      0.123110         0.012917        0.008818   \n",
       "51       2.746378      0.032168         0.018482        0.007021   \n",
       "\n",
       "   param_max_depth param_min_samples_leaf param_min_samples_split  \\\n",
       "49              16                      1                       6   \n",
       "50              16                      1                      11   \n",
       "51              16                      1                      16   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "49  {'max_depth': 16, 'min_samples_leaf': 1, 'min_...           0.859753   \n",
       "50  {'max_depth': 16, 'min_samples_leaf': 1, 'min_...           0.858634   \n",
       "51  {'max_depth': 16, 'min_samples_leaf': 1, 'min_...           0.857650   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "49           0.863576           0.860023           0.863193   \n",
       "50           0.862580           0.858585           0.862554   \n",
       "51           0.861154           0.857589           0.861189   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "49           0.858608         0.861031        0.001984                1  \n",
       "50           0.857784         0.860027        0.002096                2  \n",
       "51           0.856272         0.858771        0.002021                3  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# soppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set the hyperparameters to search for the decision tree\n",
    "dtc_param = {'max_depth':np.arange(1, 20, 5), \n",
    "                  'min_samples_split':np.arange(1, 20, 5),\n",
    "                  'min_samples_leaf':np.arange(1, 20, 5)}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(dtc, dtc_param, cv = 5,)\n",
    "clf.fit(X_train_scl, y_train)\n",
    "# display the top ranked models \n",
    "pd.DataFrame(clf.cv_results_).sort_values(by = 'rank_test_score').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier():\n",
      "{'max_depth': 16, 'min_samples_leaf': 1, 'min_samples_split': 6}\n"
     ]
    }
   ],
   "source": [
    "print(f'{dtc}:\\n{clf.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=16, min_samples_split=6) \n",
      "Test mean accuracy: 0.8642\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth = 16, min_samples_leaf = 1, min_samples_split = 6) \n",
    "dtc.fit(X_train_scl,y_train) \n",
    "print(dtc, f'\\nTest mean accuracy: {dtc.score(X_test_scl,y_test):.4f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier() \n",
      "Test mean accuracy: 0.9361\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier() \n",
    "dtc.fit(X_train_scl,y_train) \n",
    "print(dtc, f'\\nTest mean accuracy: {dtc.score(X_test_scl,y_test):.4f}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('book')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdb129d5228a765a304b8d5599f04ed0b9473866110291e13ce84f812c5eadab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
