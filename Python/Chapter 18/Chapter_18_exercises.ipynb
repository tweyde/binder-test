{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18: Regression Models and Evaluation\n",
    "## Exercises solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Inspect the coefficients of the linear model trained on the polynomial features. Find the coefficient responsible for the large error and explain why it has an especially strong effect on the polynomial features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will use the *diabetes* dataset: this dataset has 10 attributes about a patient as features and disease progression after a year as the target. The dataset contains data for 442 patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the modules\n",
    "import sklearn\n",
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset and analyse its attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset type: \n",
      "<class 'sklearn.utils.Bunch'>\n",
      "\n",
      "Dataset attributes: \n",
      "dict_keys(['data', 'target', 'frame', 'DESCR', 'feature_names', 'data_filename', 'target_filename', 'data_module'])\n",
      "\n",
      "Feaures names: \n",
      "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
      "\n",
      "Feaures file name: \n",
      "diabetes_data.csv.gz\n",
      "\n",
      "Target file name: \n",
      "diabetes_target.csv.gz\n",
      "\n",
      "Number of samples: \n",
      "442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "dataset = load_diabetes()\n",
    "# store the features in a variable X\n",
    "X = dataset.data\n",
    "# store the targets in a variable y \n",
    "y = dataset.target\n",
    "\n",
    "# we can see that this dataset is a class \n",
    "print(f'Dataset type: \\n{type(dataset)}\\n')\n",
    "# and it has some functions and attributes that describe it\n",
    "print(f'Dataset attributes: \\n{dataset.keys()}\\n')\n",
    "# these are our features (X)\n",
    "print(f'Feaures names: \\n{dataset.feature_names}\\n')\n",
    "# this is the features file name\n",
    "print(f'Feaures file name: \\n{dataset.data_filename}\\n')\n",
    "# this is the targets/labels file\n",
    "print(f'Target file name: \\n{dataset.target_filename}\\n')\n",
    "# number of samples\n",
    "print(f'Number of samples: \\n{len(dataset.data)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how, differently from the previous Sklearn datasets, the *diabetes* dataset pulls features and targets data from two different datafiles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data in training, validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the training set: 265\n",
      "Number of features in the validation set: 89\n",
      "Number of features in the test set: 88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 0 # this is used with the train_test_split to avoid random behaviour \n",
    "\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=.4,random_state=seed)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=.5,random_state=seed)\n",
    "\n",
    "print(f'Number of features in the training set: {len(X_train)}')\n",
    "print(f'Number of features in the validation set: {len(X_val)}')\n",
    "print(f'Number of features in the test set: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stadardize the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scl mean: 9.384526698718305e-18, X_train_scl std: 1.0000000000000002\n",
      "X_val_scl mean: -0.026560858245457477, X_train_scl std: 0.9586107835341086\n",
      "X_test_scl mean: -0.006411628461981172, X_test_scl std: 0.9215631575692944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sclr = StandardScaler()\n",
    "sclr.fit(X_train) # scale to 0 mean and std dev 1 on training data\n",
    "\n",
    "X_train_scl = sclr.transform(X_train) # scale all 3 sets:\n",
    "X_val_scl = sclr.transform(X_val)\n",
    "X_test_scl = sclr.transform(X_test)\n",
    "\n",
    "\n",
    "print(f'X_train_scl mean: {np.mean(X_train_scl)}, X_train_scl std: {np.std(X_train_scl)}')\n",
    "print(f'X_val_scl mean: {np.mean(X_val_scl)}, X_train_scl std: {np.std(X_val_scl)}')\n",
    "print(f'X_test_scl mean: {np.mean(X_test_scl)}, X_test_scl std: {np.std(X_test_scl)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process the data to a second polyynomial degree, so that we have more features that relate to the input in different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train data - Number of samples:  265, Number of features: 10\n",
      "X_train_sc_pf2 - Number of samples:  265, Number of features: 66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pf2 = PolynomialFeatures(degree=2)\n",
    "X_train_sc_pf2 = pf2.fit_transform(X_train_scl)\n",
    "\n",
    "print(f'X_train data - Number of samples:  {X_train.shape[0]}, Number of features: {X_train.shape[1]}')\n",
    "print(f'X_train_sc_pf2 - Number of samples:  {X_train_sc_pf2.shape[0]}, Number of features: {X_train_sc_pf2.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the available programming libraries in Python or R, linear regression is very easy to use and just like the previous models for classification we use their fit and predict methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poly2 Features, train RMSE:  44.510653384993034 , val RMSE:  67.57200276608783 , test RMSE:  63.302612578827166\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create the polynomial features from the scaled sets \n",
    "X_val_sc_pf2 = pf2.transform(X_val_scl)\n",
    "X_test_sc_pf2 = pf2.transform(X_test_scl)\n",
    "\n",
    "# train a linar model \n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(X_train_sc_pf2,y_train)\n",
    "\n",
    "# helper functions to calculate the accuracy values on train, validation and test set. \n",
    "def rmse(X,y,predictor):\n",
    "    return (mse(X,y,predictor))**.5\n",
    "\n",
    "# helper functions to calculate the accuracy values on train, validation and test set. \n",
    "def mse(X,y,predictor):\n",
    "    return ((predictor.predict(X)-y)**2).mean()\n",
    "\n",
    "def trainValTestMse(predictor):\n",
    "    vals = {}\n",
    "    vals['train'] = rmse(X_train_scl,y_train,predictor)\n",
    "    vals['val'] = rmse(X_val_scl,y_val,predictor)\n",
    "    vals['test'] = rmse(X_test_scl,y_test,predictor)\n",
    "    return vals\n",
    "\n",
    "# make predictions and print the model performance\n",
    "print('Poly2 Features, train RMSE: ', rmse(X_train_sc_pf2,y_train,lr2),\n",
    "      ', val RMSE: ' , rmse(X_val_sc_pf2,y_val,lr2),\n",
    "      ', test RMSE: ' , rmse(X_test_sc_pf2,y_test,lr2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the non-zero polynamial features and see their weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names and values of non-zero coefficients [('1', 2.5223795439938152e-11), ('x0', -0.20511584590732124), ('x1', -1524667891983.5903), ('x2', 25.040544010051313), ('x3', 13.961026894473497), ('x4', 616.9132360227866), ('x5', -550.7496843582958), ('x6', -250.38763410759816), ('x7', -6.787510444727749), ('x8', -166.05624389648438), ('x9', -2.8990478515625), ('x0^2', 4.3992919921875), ('x0 x1', 11.645872592926025), ('x0 x2', -0.1331024169921875), ('x0 x3', -1.105499267578125), ('x0 x4', 18.490509033203125), ('x0 x5', -35.373291015625), ('x0 x6', 0.9011077880859375), ('x0 x7', 14.819023132324219), ('x0 x8', -3.2235870361328125), ('x0 x9', 4.136219024658203), ('x1^2', 67335183326055.91), ('x1 x2', 4.298942565917969), ('x1 x3', 6.38946533203125), ('x1 x4', 37.453277587890625), ('x1 x5', -35.497161865234375), ('x1 x6', -4.8126220703125), ('x1 x7', 6.168281555175781), ('x1 x8', -18.69781494140625), ('x1 x9', -1.32861328125), ('x2^2', 0.92742919921875), ('x2 x3', 3.303680419921875), ('x2 x4', -2.5377197265625), ('x2 x5', -10.054847717285156), ('x2 x6', 11.7989501953125), ('x2 x7', 16.06646728515625), ('x2 x8', 6.022502899169922), ('x2 x9', -7.2200469970703125), ('x3^2', 6.927154541015625), ('x3 x4', -76.1461181640625), ('x3 x5', 70.01773071289062), ('x3 x6', 32.48463439941406), ('x3 x7', -2.488433837890625), ('x3 x8', 25.46917724609375), ('x3 x9', -7.748443603515625), ('x4^2', 302.24859619140625), ('x4 x5', -410.1386413574219), ('x4 x6', -312.3190612792969), ('x4 x7', -176.96011352539062), ('x4 x8', -333.5286865234375), ('x4 x9', -60.94465637207031), ('x5^2', 139.02743530273438), ('x5 x6', 202.0415496826172), ('x5 x7', 115.12100219726562), ('x5 x8', 249.1632080078125), ('x5 x9', 54.50048828125), ('x6^2', 93.84478759765625), ('x6 x7', 119.61395263671875), ('x6 x8', 151.4703369140625), ('x6 x9', 24.528472900390625), ('x7^2', 34.46563720703125), ('x7 x8', 64.32049560546875), ('x7 x9', 3.916107177734375), ('x8^2', 23.83916473388672), ('x8 x9', 20.477493286132812), ('x9^2', 8.383834838867188)]\n"
     ]
    }
   ],
   "source": [
    "nz_feature_names = pf2.get_feature_names_out()[np.nonzero(lr2.coef_)]\n",
    "nz_feature_values = lr2.coef_[np.nonzero(lr2.coef_)]\n",
    "zipped = list(zip(nz_feature_names,nz_feature_values))\n",
    "print('Names and values of non-zero coefficients', zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x1^2', 67335183326055.91),\n",
       " ('x4', 616.9132360227866),\n",
       " ('x4^2', 302.24859619140625),\n",
       " ('x5 x8', 249.1632080078125),\n",
       " ('x5 x6', 202.0415496826172),\n",
       " ('x6 x8', 151.4703369140625),\n",
       " ('x5^2', 139.02743530273438),\n",
       " ('x6 x7', 119.61395263671875),\n",
       " ('x5 x7', 115.12100219726562),\n",
       " ('x6^2', 93.84478759765625),\n",
       " ('x3 x5', 70.01773071289062),\n",
       " ('x7 x8', 64.32049560546875),\n",
       " ('x5 x9', 54.50048828125),\n",
       " ('x1 x4', 37.453277587890625),\n",
       " ('x7^2', 34.46563720703125),\n",
       " ('x3 x6', 32.48463439941406),\n",
       " ('x3 x8', 25.46917724609375),\n",
       " ('x2', 25.040544010051313),\n",
       " ('x6 x9', 24.528472900390625),\n",
       " ('x8^2', 23.83916473388672),\n",
       " ('x8 x9', 20.477493286132812),\n",
       " ('x0 x4', 18.490509033203125),\n",
       " ('x2 x7', 16.06646728515625),\n",
       " ('x0 x7', 14.819023132324219),\n",
       " ('x3', 13.961026894473497),\n",
       " ('x2 x6', 11.7989501953125),\n",
       " ('x0 x1', 11.645872592926025),\n",
       " ('x9^2', 8.383834838867188),\n",
       " ('x3^2', 6.927154541015625),\n",
       " ('x1 x3', 6.38946533203125),\n",
       " ('x1 x7', 6.168281555175781),\n",
       " ('x2 x8', 6.022502899169922),\n",
       " ('x0^2', 4.3992919921875),\n",
       " ('x1 x2', 4.298942565917969),\n",
       " ('x0 x9', 4.136219024658203),\n",
       " ('x7 x9', 3.916107177734375),\n",
       " ('x2 x3', 3.303680419921875),\n",
       " ('x2^2', 0.92742919921875),\n",
       " ('x0 x6', 0.9011077880859375),\n",
       " ('1', 2.5223795439938152e-11),\n",
       " ('x0 x2', -0.1331024169921875),\n",
       " ('x0', -0.20511584590732124),\n",
       " ('x0 x3', -1.105499267578125),\n",
       " ('x1 x9', -1.32861328125),\n",
       " ('x3 x7', -2.488433837890625),\n",
       " ('x2 x4', -2.5377197265625),\n",
       " ('x9', -2.8990478515625),\n",
       " ('x0 x8', -3.2235870361328125),\n",
       " ('x1 x6', -4.8126220703125),\n",
       " ('x7', -6.787510444727749),\n",
       " ('x2 x9', -7.2200469970703125),\n",
       " ('x3 x9', -7.748443603515625),\n",
       " ('x2 x5', -10.054847717285156),\n",
       " ('x1 x8', -18.69781494140625),\n",
       " ('x0 x5', -35.373291015625),\n",
       " ('x1 x5', -35.497161865234375),\n",
       " ('x4 x9', -60.94465637207031),\n",
       " ('x3 x4', -76.1461181640625),\n",
       " ('x8', -166.05624389648438),\n",
       " ('x4 x7', -176.96011352539062),\n",
       " ('x6', -250.38763410759816),\n",
       " ('x4 x6', -312.3190612792969),\n",
       " ('x4 x8', -333.5286865234375),\n",
       " ('x4 x5', -410.1386413574219),\n",
       " ('x5', -550.7496843582958),\n",
       " ('x1', -1524667891983.5903)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the features by their coefficients weights\n",
    "zipped.sort(key=lambda x: x[1], reverse = True)\n",
    "zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is particularly evident how some features'coefficients tend to \"explode\" (e.g., 'x1^2': 67335183326055.91 and 'x1': -1524667891983.5903) in the model's research of fitting each data point exactly. This is the typical behaviour of a model **overfitting** the traininig data, therefore incapable of generalising on unseen data and likely to yield much big errros on validation and test sets. This issue with high-order polynomials tends to get worse with the increasing of the polynomial order. <br>Standard solutions to this problem are regularizations methods such as *L1 (Lasso)* and *L2 (Ridge)* which add a penalty term to the model's *cost function* proportional to the size of the coefficients, hence penalizing large values. Namely, the L1 regularization adds a *penalty* equal to the absolute value of the magnitude of the coefficients, whereas the L2 regularization one equal to the square of the magnitude. These techniques can help reduce *variance* in the model and yield smaller generalization errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Implement a grid search for the alpha parameter of a linear regression on the train/validation split with the poly 2 features of the diabetes data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# we store trainig and validation scores here\n",
    "scores_t = []; scores_v = [] \n",
    "\n",
    "# set the range of values for our hyperparameter\n",
    "params = np.arange(0, 100)\n",
    "\n",
    "# test the KNN at different number of neighbors and append the results to the above lists\n",
    "for p in params:\n",
    "    lr_rd = Ridge(alpha = p, fit_intercept=True)\n",
    "    lr_rd.fit(X_train_sc_pf2, y_train)\n",
    "    scores_t.append(rmse(X_train_sc_pf2, y_train, lr_rd))\n",
    "    scores_v.append(rmse(X_val_sc_pf2, y_val, lr_rd))\n",
    "\n",
    "# plot the training and validation curves \n",
    "plt.title(\"Training and Validation Curves at different α\")\n",
    "plt.xlabel(\"α\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.ylim(0, 100)\n",
    "plt.plot(params, scores_t, label=\"Train RMSE\")\n",
    "plt.plot(params, scores_v, label=\"Validation RMSE\")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice how larger values of α for the *Ridge (L2)* regularization loss reduce the model's *variance* resulting in progressively higher errors on the trainig data but a better generalization error on the validation data. Regularization techniques can help us control what in ML is described as *bias-variance trade-off* of a model, which relates to its degree of fitting the trainig and validation data (*underfitting-overfitting*). It's the operator's task to find the sweat-spot where the model performs at the desired/required accuracy level on either or both the trainig and validation data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Implement a grid search for the alpha parameter of a linear regression using cross validation with the poly 2 features of the diabetes data. Use the Scikit-Learn class GridSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_MSE</th>\n",
       "      <th>split1_test_MSE</th>\n",
       "      <th>split2_test_MSE</th>\n",
       "      <th>split3_test_MSE</th>\n",
       "      <th>split4_test_MSE</th>\n",
       "      <th>mean_test_MSE</th>\n",
       "      <th>std_test_MSE</th>\n",
       "      <th>rank_test_MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>67</td>\n",
       "      <td>{'alpha': 67}</td>\n",
       "      <td>-51.246532</td>\n",
       "      <td>-56.870821</td>\n",
       "      <td>-64.272535</td>\n",
       "      <td>-55.076004</td>\n",
       "      <td>-56.220974</td>\n",
       "      <td>-56.737373</td>\n",
       "      <td>4.241995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68</td>\n",
       "      <td>{'alpha': 68}</td>\n",
       "      <td>-51.261669</td>\n",
       "      <td>-56.861426</td>\n",
       "      <td>-64.252735</td>\n",
       "      <td>-55.066144</td>\n",
       "      <td>-56.245864</td>\n",
       "      <td>-56.737568</td>\n",
       "      <td>4.231170</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>66</td>\n",
       "      <td>{'alpha': 66}</td>\n",
       "      <td>-51.231746</td>\n",
       "      <td>-56.880644</td>\n",
       "      <td>-64.292808</td>\n",
       "      <td>-55.086603</td>\n",
       "      <td>-56.196060</td>\n",
       "      <td>-56.737572</td>\n",
       "      <td>4.252884</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "67       0.000209      0.000418         0.000621        0.000508          67   \n",
       "68       0.000600      0.000490         0.000000        0.000000          68   \n",
       "66       0.000435      0.000538         0.000200        0.000400          66   \n",
       "\n",
       "           params  split0_test_MSE  split1_test_MSE  split2_test_MSE  \\\n",
       "67  {'alpha': 67}       -51.246532       -56.870821       -64.272535   \n",
       "68  {'alpha': 68}       -51.261669       -56.861426       -64.252735   \n",
       "66  {'alpha': 66}       -51.231746       -56.880644       -64.292808   \n",
       "\n",
       "    split3_test_MSE  split4_test_MSE  mean_test_MSE  std_test_MSE  \\\n",
       "67       -55.076004       -56.220974     -56.737373      4.241995   \n",
       "68       -55.066144       -56.245864     -56.737568      4.231170   \n",
       "66       -55.086603       -56.196060     -56.737572      4.252884   \n",
       "\n",
       "    rank_test_MSE  \n",
       "67              1  \n",
       "68              2  \n",
       "66              3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# implemente a linear least squares with l2 regularization\n",
    "lr_rd = Ridge(fit_intercept=True)\n",
    "\n",
    "# check alpha parameters between 0 and 100 \n",
    "parameters = {'alpha':np.arange(0, 100)}\n",
    "\n",
    "# in sklearn all scorer objects follow the convention that higher return values are better than lower return values. \n",
    "# Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error, are available as neg_mean_squared_error.\n",
    "scoring = {'MSE': 'neg_root_mean_squared_error'}\n",
    "\n",
    "# set a 5 fold nested grid search on the poly2 features\n",
    "clf = GridSearchCV(lr_rd, parameters, scoring = scoring, refit = False, cv = 5)\n",
    "clf.fit(X_train_sc_pf2, y_train)\n",
    "\n",
    "# display the top ranked models \n",
    "pd.DataFrame(clf.cv_results_).sort_values(by = 'rank_test_MSE').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE:  48.0190692372308\n",
      "Val RMSE:  54.53498875140474\n"
     ]
    }
   ],
   "source": [
    "lr_rd = Ridge(alpha = 67, fit_intercept=True)\n",
    "lr_rd.fit(X_train_sc_pf2, y_train)\n",
    "\n",
    "print('Train RMSE: ', rmse(X_train_sc_pf2, y_train,lr_rd),)\n",
    "print('Val RMSE: ', rmse(X_val_sc_pf2, y_val,lr_rd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through a nested grid search we can find the value of α that yields the best model performance on the given data. To notice how the model performance varies on the splits (look at *split2*) which highliths how training and testing a model is a complex matter, where all the moving parts can have a significant impact on the final result. E.g., if we tested this model on differnt splits, would we alwasy get the same optimal α?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Load the California Housing dataset (either with sklearn.datasets.fetch_california_housing or from here ) and predict the house price (as a regression). Use different models, expand the features, tune the hyper-parameters, select the best model and estimate its performance on new data. Justify the choices or methods and models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset type: \n",
      "<class 'sklearn.utils.Bunch'>\n",
      "\n",
      "Dataset attributes: \n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])\n",
      "\n",
      "Feaures names: \n",
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "\n",
      "Target name: \n",
      "['MedHouseVal']\n",
      "\n",
      "Number of samples: \n",
      "20640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "df = sklearn.datasets.fetch_california_housing()\n",
    "\n",
    "# this dataset is a class \n",
    "print(f'Dataset type: \\n{type(df)}\\n')\n",
    "# and it has some functions and attributes that describe it\n",
    "print(f'Dataset attributes: \\n{df.keys()}\\n')\n",
    "# these are our features (X)\n",
    "print(f'Feaures names: \\n{df.feature_names}\\n')\n",
    "# this is our target (y)\n",
    "print(f'Target name: \\n{df.target_names}\\n')\n",
    "# number of samples\n",
    "print(f'Number of samples: \\n{len(df.data)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the training set: 14448\n",
      "Number of targets in the training set: 14448\n",
      "Number of features in the test set: 6192\n",
      "Number of targtes in the test set: 6192\n"
     ]
    }
   ],
   "source": [
    "# assign features and targets to X and y\n",
    "X = df.data\n",
    "y = df.target\n",
    "\n",
    "# split features and targets in train and test sets (). Note how we can easily stratify the sets. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = seed)\n",
    "\n",
    "print(f'Number of features in the training set: {len(X_train)}')\n",
    "print(f'Number of targets in the training set: {len(y_train)}')\n",
    "print(f'Number of features in the test set: {len(X_test)}')\n",
    "print(f'Number of targtes in the test set: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scl mean: -7.349110860901894e-15, X_train_scl std: 1.000000000000002\n",
      "X_test_scl mean: 0.005991271762881557, X_test_scl std: 1.2785474136594306\n"
     ]
    }
   ],
   "source": [
    "sclr.fit(X_train) # scale to 0 mean and std dev 1 on training data\n",
    "\n",
    "X_train_scl = sclr.transform(X_train) # scale all 3 sets:\n",
    "X_test_scl = sclr.transform(X_test)\n",
    "\n",
    "\n",
    "print(f'X_train_scl mean: {np.mean(X_train_scl)}, X_train_scl std: {np.std(X_train_scl)}')\n",
    "print(f'X_test_scl mean: {np.mean(X_test_scl)}, X_test_scl std: {np.std(X_test_scl)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Training\n",
    "Let us compare a few regression models:\n",
    "- Ordinary least squares (OLS)\n",
    "- Lasso \n",
    "- Ridge \n",
    "- Decision tree\n",
    "- Multy-layer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit and train a \"vanilla\" version of the models on the data. No hyperparameters tuning, no expanded features, all very basic for now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(): \n",
      "Train RMSE: 0.719 - Test RMSE: 0.737\n",
      "Lasso(): \n",
      "Train RMSE: 1.154 - Test RMSE: 1.155\n",
      "Ridge(): \n",
      "Train RMSE: 0.719 - Test RMSE: 0.737\n",
      "DecisionTreeRegressor(): \n",
      "Train RMSE: 0.000 - Test RMSE: 0.733\n",
      "MLPRegressor(max_iter=500): \n",
      "Train RMSE: 0.522 - Test RMSE: 0.546\n"
     ]
    }
   ],
   "source": [
    "# import the models \n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# OLS\n",
    "ols = LinearRegression()\n",
    "# Lasso\n",
    "lasso = Lasso()\n",
    "# Ridge\n",
    "ridge = Ridge()\n",
    "# Decision tree\n",
    "dt = DecisionTreeRegressor()\n",
    "# MLP\n",
    "mlp = MLPRegressor(max_iter = 500)\n",
    "# store the models in a list of models \n",
    "models = [ols, lasso, ridge, dt, mlp]\n",
    "# loop through the above list to fit, train and test the models\n",
    "for model in models:\n",
    "    model.fit(X_train_scl, y_train)\n",
    "    print(f'{model}: \\nTrain RMSE:',f'{rmse(X_train_scl, y_train, model):.3f}', \n",
    "                    '- Test RMSE:', f'{rmse(X_test_scl, y_test, model):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice how the MLP was the best peformer in this scenario. Also interesting to see how the decision tree perfectly fitted the training data (trees tend to *overfit* if not tuned). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models training on polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us expand the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create polynomial features for the scaled training and test features\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "X_train_pf_scl = pf.fit_transform(X_train_scl)\n",
    "X_test_pf_scl = pf.transform(X_test_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(): \n",
      "Train RMSE: 0.646 - Test RMSE: 1.462\n",
      "Lasso(): \n",
      "Train RMSE: 1.132 - Test RMSE: 1.133\n",
      "Ridge(): \n",
      "Train RMSE: 0.646 - Test RMSE: 1.428\n",
      "DecisionTreeRegressor(): \n",
      "Train RMSE: 0.000 - Test RMSE: 0.742\n",
      "MLPRegressor(max_iter=500): \n",
      "Train RMSE: 0.958 - Test RMSE: 1.807\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    model.fit(X_train_pf_scl, y_train)\n",
    "    print(f'{model}: \\nTrain RMSE:',f'{rmse(X_train_pf_scl, y_train, model):.3f}', \n",
    "                    '- Test RMSE:', f'{rmse(X_test_pf_scl, y_test, model):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see how the *linear models* respond well to training on the polynomial features, altough they overfit the data more heavily and will require more extensive hyperparameter tuning to transfer that learning to the test data. Differently, MLP (non-linear) does not like that features engineering at all... <br> By looking at the above results, in accordance with the theory, we can try and optimize the linear models (OLS, Lasso and Ridge) on the polynomial features and the *non-linear* models (Decision Tree and MLP on the original features):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models fine tuning\n",
    "We set the hyperparameters space for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "lasso_param = {'alpha':np.arange(1, 100)}\n",
    "# Ridge\n",
    "ridge_param = {'alpha':np.arange(1, 100)}\n",
    "# Decision tree\n",
    "dt_param = {'max_depth':np.arange(1, 20, 4), \n",
    "            'min_samples_split':np.arange(2, 8, 2),\n",
    "            'min_samples_leaf':np.arange(1, 20, 4)}\n",
    "\n",
    "# MLP - we can specify its architecture by defining the number of neurons at each layer \n",
    "mlp_param = {'hidden_layer_sizes': [(100,), (100, 100)],\n",
    "             'max_iter': [500]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a nested gridsearch for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(): \n",
      "Train RMSE: 1.1320566676355657 - Best parameters: {'alpha': 1}\n",
      "Ridge(): \n",
      "Train RMSE: 0.68844599485411 - Best parameters: {'alpha': 99}\n",
      "DecisionTreeRegressor(): \n",
      "Train RMSE: 0.6205940829038027 - Best parameters: {'max_depth': 17, 'min_samples_leaf': 17, 'min_samples_split': 6}\n",
      "MLPRegressor(max_iter=500): \n",
      "Train RMSE: 0.5256631972046046 - Best parameters: {'hidden_layer_sizes': (100, 100), 'max_iter': 500}\n"
     ]
    }
   ],
   "source": [
    "# in sklearn all scorer objects follow the convention that higher return values are better than lower return values. \n",
    "# Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error, are available as neg_mean_squared_error.\n",
    "scoring = {'MSE': 'neg_root_mean_squared_error'}\n",
    "# store the models \n",
    "models = [lasso, ridge, dt, mlp]\n",
    "# store the models parameters \n",
    "models_parameters = [lasso_param, ridge_param, dt_param, mlp_param]\n",
    "\n",
    "# loop through each model, and their paramaters to estimate, and run a grid search for each of them \n",
    "for m in range(len(models)):\n",
    "    grid = GridSearchCV(models[m], models_parameters[m], scoring = scoring, refit = False, cv = 5)\n",
    "    # fit the lasso and ridge models to the polynomial scaled features \n",
    "    if m <= 1:\n",
    "        grid.fit(X_train_pf_scl, y_train)\n",
    "    # fit the deciosion tree and the MLP to the original scaled features \n",
    "    else:\n",
    "        grid.fit(X_train_scl, y_train)\n",
    "    # extract the best model \n",
    "    model_df = pd.DataFrame(grid.cv_results_).sort_values(by = 'rank_test_MSE').head(1)\n",
    "    print(f'{models[m]}: \\nTrain RMSE: {-model_df.mean_test_MSE.values[0]} - Best parameters: {model_df.params.values[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now re-train the above models with the above hyperparameters and see how they perform on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=1): \n",
      "Train RMSE: 1.132 - Test RMSE: 1.133\n",
      "Ridge(alpha=99): \n",
      "Train RMSE: 0.654 - Test RMSE: 0.692\n",
      "DecisionTreeRegressor(max_depth=17, min_samples_leaf=17, min_samples_split=6): \n",
      "Train RMSE: 0.491 - Test RMSE: 0.621\n",
      "MLPRegressor(max_iter=500): \n",
      "Train RMSE: 0.517 - Test RMSE: 0.538\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha = 1)\n",
    "ridge = Ridge(alpha = 99)\n",
    "dt = DecisionTreeRegressor(max_depth = 17, min_samples_leaf = 17, min_samples_split = 6)\n",
    "MLP = MLPRegressor(hidden_layer_sizes = (100, 100), max_iter = 500)\n",
    "\n",
    "models = [lasso, ridge, dt, mlp]\n",
    "\n",
    "for m in range(len(models)):\n",
    "    if m <= 1:\n",
    "        models[m].fit(X_train_pf_scl, y_train)\n",
    "        print(f'{models[m]}: \\nTrain RMSE:',f'{rmse(X_train_pf_scl, y_train, models[m]):.3f}', \n",
    "                    '- Test RMSE:', f'{rmse(X_test_pf_scl, y_test, models[m]):.3f}')\n",
    "    # fit the deciosion tree and the MLP to the original scaled features \n",
    "    else:\n",
    "        models[m].fit(X_train_scl, y_train)\n",
    "        print(f'{models[m]}: \\nTrain RMSE:',f'{rmse(X_train_scl, y_train, models[m]):.3f}', \n",
    "                    '- Test RMSE:', f'{rmse(X_test_scl, y_test, models[m]):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the above results to first ones we obtained from the *vanilla* models on the standard features we can see how we managed to lower the error on the test set for each model! <br>\n",
    "Below we re-write some code to facilitate the comparison bertween the untuned and tuned models (results may vary when re-running the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UNTUNED MODELS === \n",
      "\n",
      "Lasso(): \n",
      "Train RMSE: 1.154 - Test RMSE: 1.155\n",
      "Ridge(): \n",
      "Train RMSE: 0.719 - Test RMSE: 0.737\n",
      "DecisionTreeRegressor(): \n",
      "Train RMSE: 0.000 - Test RMSE: 0.737\n",
      "MLPRegressor(max_iter=500): \n",
      "Train RMSE: 0.530 - Test RMSE: 0.572\n",
      "\n",
      "=== TUNED MODELS === \n",
      "\n",
      "Lasso(alpha=1): \n",
      "Train RMSE: 1.132 - Test RMSE: 1.133\n",
      "Ridge(alpha=99): \n",
      "Train RMSE: 0.654 - Test RMSE: 0.692\n",
      "DecisionTreeRegressor(max_depth=17, min_samples_leaf=17, min_samples_split=6): \n",
      "Train RMSE: 0.491 - Test RMSE: 0.621\n",
      "MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=500): \n",
      "Train RMSE: 0.428 - Test RMSE: 0.534\n"
     ]
    }
   ],
   "source": [
    "### UNTUNED MODELS ###\n",
    "lasso = Lasso()\n",
    "# Ridge\n",
    "ridge = Ridge()\n",
    "# Decision tree\n",
    "dt = DecisionTreeRegressor()\n",
    "# MLP\n",
    "mlp = MLPRegressor(max_iter = 500)\n",
    "# store the models in a list of models \n",
    "models = [lasso, ridge, dt, mlp]\n",
    "# loop through the above list to fit, train and test the models\n",
    "print('=== UNTUNED MODELS === \\n')\n",
    "for model in models:\n",
    "    model.fit(X_train_scl, y_train)\n",
    "    print(f'{model}: \\nTrain RMSE:',f'{rmse(X_train_scl, y_train, model):.3f}', \n",
    "                    '- Test RMSE:', f'{rmse(X_test_scl, y_test, model):.3f}')\n",
    "\n",
    "### TUNED MODELS ###\n",
    "lasso = Lasso(alpha = 1)\n",
    "ridge = Ridge(alpha = 99)\n",
    "dt = DecisionTreeRegressor(max_depth = 17, min_samples_leaf = 17, min_samples_split = 6)\n",
    "mlp = MLPRegressor(hidden_layer_sizes = (100, 100), max_iter = 500)\n",
    "\n",
    "models = [lasso, ridge, dt, mlp]\n",
    "print('\\n=== TUNED MODELS === \\n')\n",
    "for m in range(len(models)):\n",
    "    if m <= 1:\n",
    "        models[m].fit(X_train_pf_scl, y_train)\n",
    "        print(f'{models[m]}: \\nTrain RMSE:',f'{rmse(X_train_pf_scl, y_train, models[m]):.3f}', \n",
    "                    '- Test RMSE:', f'{rmse(X_test_pf_scl, y_test, models[m]):.3f}')\n",
    "    # fit the deciosion tree and the MLP to the original scaled features \n",
    "    else:\n",
    "        models[m].fit(X_train_scl, y_train)\n",
    "        print(f'{models[m]}: \\nTrain RMSE:',f'{rmse(X_train_scl, y_train, models[m]):.3f}', \n",
    "                    '- Test RMSE:', f'{rmse(X_test_scl, y_test, models[m]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Load the Temperature Prediction Bias dataset from https://archive.ics.uci.edu/ml/datasets/Bias+correction+of+numerical+prediction+model+temperature+forecast and predict the the difference between predicted and actual next day temperatures. Model this regression problem with different models and determine their optimal hyper-parameters, select the best model and estimate its performance on new data. Justify the choices or methods and models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is similar to the previous one so we will use the same logic: \n",
    "1. We will prepare the data\n",
    "2. Train the vanilla models \n",
    "3. Tune the models \n",
    "4. Compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Information\n",
    "This data is for the purpose of bias correction of next-day maximum and minimum air temperatures forecast of the LDAPS model operated by the Korea Meteorological Administration over Seoul, South Korea. This data consists of summer data from 2013 to 2017. The input data is largely composed of the LDAPS model's next-day forecast data, in-situ maximum and minimum temperatures of present-day, and geographic auxiliary variables. There are two outputs (i.e. next-day maximum and minimum air temperatures) in this data. Hindcast validation was conducted for the period from 2015 to 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset attributes: \n",
      "Index(['station', 'Date', 'Present_Tmax', 'Present_Tmin', 'LDAPS_RHmin',\n",
      "       'LDAPS_RHmax', 'LDAPS_Tmax_lapse', 'LDAPS_Tmin_lapse', 'LDAPS_WS',\n",
      "       'LDAPS_LH', 'LDAPS_CC1', 'LDAPS_CC2', 'LDAPS_CC3', 'LDAPS_CC4',\n",
      "       'LDAPS_PPT1', 'LDAPS_PPT2', 'LDAPS_PPT3', 'LDAPS_PPT4', 'lat', 'lon',\n",
      "       'DEM', 'Slope', 'Solar radiation', 'Next_Tmax', 'Next_Tmin'],\n",
      "      dtype='object')\n",
      "\n",
      "Number of samples: \n",
      "7752\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>Date</th>\n",
       "      <th>Present_Tmax</th>\n",
       "      <th>Present_Tmin</th>\n",
       "      <th>LDAPS_RHmin</th>\n",
       "      <th>LDAPS_RHmax</th>\n",
       "      <th>LDAPS_Tmax_lapse</th>\n",
       "      <th>LDAPS_Tmin_lapse</th>\n",
       "      <th>LDAPS_WS</th>\n",
       "      <th>LDAPS_LH</th>\n",
       "      <th>...</th>\n",
       "      <th>LDAPS_PPT2</th>\n",
       "      <th>LDAPS_PPT3</th>\n",
       "      <th>LDAPS_PPT4</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>DEM</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Solar radiation</th>\n",
       "      <th>Next_Tmax</th>\n",
       "      <th>Next_Tmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>28.7</td>\n",
       "      <td>21.4</td>\n",
       "      <td>58.255688</td>\n",
       "      <td>91.116364</td>\n",
       "      <td>28.074101</td>\n",
       "      <td>23.006936</td>\n",
       "      <td>6.818887</td>\n",
       "      <td>69.451805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.6046</td>\n",
       "      <td>126.991</td>\n",
       "      <td>212.3350</td>\n",
       "      <td>2.7850</td>\n",
       "      <td>5992.895996</td>\n",
       "      <td>29.1</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>31.9</td>\n",
       "      <td>21.6</td>\n",
       "      <td>52.263397</td>\n",
       "      <td>90.604721</td>\n",
       "      <td>29.850689</td>\n",
       "      <td>24.035009</td>\n",
       "      <td>5.691890</td>\n",
       "      <td>51.937448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.6046</td>\n",
       "      <td>127.032</td>\n",
       "      <td>44.7624</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>5869.312500</td>\n",
       "      <td>30.5</td>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>31.6</td>\n",
       "      <td>23.3</td>\n",
       "      <td>48.690479</td>\n",
       "      <td>83.973587</td>\n",
       "      <td>30.091292</td>\n",
       "      <td>24.565633</td>\n",
       "      <td>6.138224</td>\n",
       "      <td>20.573050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.5776</td>\n",
       "      <td>127.058</td>\n",
       "      <td>33.3068</td>\n",
       "      <td>0.2661</td>\n",
       "      <td>5863.555664</td>\n",
       "      <td>31.1</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   station        Date  Present_Tmax  Present_Tmin  LDAPS_RHmin  LDAPS_RHmax  \\\n",
       "0      1.0  2013-06-30          28.7          21.4    58.255688    91.116364   \n",
       "1      2.0  2013-06-30          31.9          21.6    52.263397    90.604721   \n",
       "2      3.0  2013-06-30          31.6          23.3    48.690479    83.973587   \n",
       "\n",
       "   LDAPS_Tmax_lapse  LDAPS_Tmin_lapse  LDAPS_WS   LDAPS_LH  ...  LDAPS_PPT2  \\\n",
       "0         28.074101         23.006936  6.818887  69.451805  ...         0.0   \n",
       "1         29.850689         24.035009  5.691890  51.937448  ...         0.0   \n",
       "2         30.091292         24.565633  6.138224  20.573050  ...         0.0   \n",
       "\n",
       "   LDAPS_PPT3  LDAPS_PPT4      lat      lon       DEM   Slope  \\\n",
       "0         0.0         0.0  37.6046  126.991  212.3350  2.7850   \n",
       "1         0.0         0.0  37.6046  127.032   44.7624  0.5141   \n",
       "2         0.0         0.0  37.5776  127.058   33.3068  0.2661   \n",
       "\n",
       "   Solar radiation  Next_Tmax  Next_Tmin  \n",
       "0      5992.895996       29.1       21.2  \n",
       "1      5869.312500       30.5       22.5  \n",
       "2      5863.555664       31.1       23.9  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00514/Bias_correction_ucl.csv')\n",
    "\n",
    "# columns names\n",
    "print(f'Dataset attributes: \\n{df.keys()}\\n')\n",
    "# number of samples\n",
    "print(f'Number of samples: \\n{len(df)}\\n')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station              2\n",
       "Date                 2\n",
       "Present_Tmax        70\n",
       "Present_Tmin        70\n",
       "LDAPS_RHmin         75\n",
       "LDAPS_RHmax         75\n",
       "LDAPS_Tmax_lapse    75\n",
       "LDAPS_Tmin_lapse    75\n",
       "LDAPS_WS            75\n",
       "LDAPS_LH            75\n",
       "LDAPS_CC1           75\n",
       "LDAPS_CC2           75\n",
       "LDAPS_CC3           75\n",
       "LDAPS_CC4           75\n",
       "LDAPS_PPT1          75\n",
       "LDAPS_PPT2          75\n",
       "LDAPS_PPT3          75\n",
       "LDAPS_PPT4          75\n",
       "lat                  0\n",
       "lon                  0\n",
       "DEM                  0\n",
       "Slope                0\n",
       "Solar radiation      0\n",
       "Next_Tmax           27\n",
       "Next_Tmin           27\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks for missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to deal with those missing values could be by *back-filling* or *forward-filling* them, like so:\n",
    "- df = df.fillna('bfill')\n",
    "- df = df.fillna('ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since they are very few though we will just remove them in this case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation \n",
    "We start by taking care of the *Date* column. There are many ways to deal with dates in ML problems and depending on the data, problem, and degree of precision required these can be more or less involved. In this exercise we will use one of the simplest approaches and just convert the dates to ordinal numbers:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "# convert the date colums to a datetuime object \n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "# convert the dates to ordinal numbers \n",
    "df['Date']= df['Date'].map(dt.datetime.toordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This is a multi-target regression problem since we are trying to predict the next day minimum and maximum temperature (*Next_Tmin, Next_Tmax*). <br> We assing **two targets** to y, which is now a bidimensional array (*matrix*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in the training set: 5311\n",
      "Number of targets in the training set: 5311\n",
      "Number of features in the test set: 2277\n",
      "Number of targtes in the test set: 2277\n"
     ]
    }
   ],
   "source": [
    "# assign features and targets to X and y\n",
    "X = df.iloc[:, :-2].values\n",
    "y = df.iloc[:, -2:].values\n",
    "\n",
    "# split features and targets in train and test sets (). Note how we can easily stratify the sets. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = seed)\n",
    "\n",
    "print(f'Number of features in the training set: {len(X_train)}')\n",
    "print(f'Number of targets in the training set: {len(y_train)}')\n",
    "print(f'Number of features in the test set: {len(X_test)}')\n",
    "print(f'Number of targtes in the test set: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stadardize the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scl mean: 2.2368448157803305e-12, X_train_scl std: 0.9999999999999986\n",
      "X_test_scl mean: 0.009333647463976257, X_test_scl std: 1.0064106348684931\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sclr = StandardScaler()\n",
    "sclr.fit(X_train) # scale to 0 mean and std dev 1 on training data\n",
    "\n",
    "X_train_scl = sclr.transform(X_train) # scale all 3 sets:\n",
    "X_test_scl = sclr.transform(X_test)\n",
    "\n",
    "\n",
    "print(f'X_train_scl mean: {np.mean(X_train_scl)}, X_train_scl std: {np.std(X_train_scl)}')\n",
    "print(f'X_test_scl mean: {np.mean(X_test_scl)}, X_test_scl std: {np.std(X_test_scl)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Training\n",
    "Let us compare a few regression models:\n",
    "\n",
    "Ordinary least squares (OLS)\n",
    "Lasso\n",
    "Ridge\n",
    "Decision tree\n",
    "Multy-layer perceptron (MLP)\n",
    "We fit and train a \"vanilla\" version of the models on the data. No hyperparameters tuning, no expanded features, all very basic for now. <br> Note the use of **MultiOutputRegressor()** to wrap our models and make them support multi-target regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(): \n",
      "Train RMSE: 1.752 - Test RMSE: 1.778\n",
      "Ridge(): \n",
      "Train RMSE: 1.249 - Test RMSE: 1.246\n",
      "DecisionTreeRegressor(): \n",
      "Train RMSE: 0.000 - Test RMSE: 1.287\n",
      "MLPRegressor(max_iter=1000): \n",
      "Train RMSE: 0.518 - Test RMSE: 0.827\n"
     ]
    }
   ],
   "source": [
    "# import the models \n",
    "from sklearn.linear_model import  Ridge, LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Lasso\n",
    "lasso = Lasso()\n",
    "# Ridge\n",
    "ridge = Ridge()\n",
    "# Decision tree\n",
    "dt = DecisionTreeRegressor()\n",
    "# MLP\n",
    "mlp = MLPRegressor(max_iter = 1000)\n",
    "# store the models in a list of models \n",
    "models = [lasso, ridge, dt, mlp]\n",
    "# loop through the above list to fit, train and test the models\n",
    "for model in models:\n",
    "    regr = MultiOutputRegressor(model).fit(X_train_scl, y_train)\n",
    "    print(f'{model}: \\nTrain RMSE:',f'{rmse(X_train_scl, y_train, regr):.3f}', \n",
    "                    '- Test RMSE:', f'{rmse(X_test_scl, y_test, regr):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the models now make a double prediction (i.e., on *Next_Tmax, Next_Tmin*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [30.10738751 20.62798573]\n",
      "Real values: [30.6 20.1]\n"
     ]
    }
   ],
   "source": [
    "# make a prediction on the first sample\n",
    "print(f'Prediction: {(regr.predict(X_train_scl[:1])[0])}')\n",
    "# show the first sample\n",
    "print(f'Real values: {y_train[:1][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us expand the features for the linear models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create polynomial features for the scaled training and test features\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "X_train_pf_scl = pf.fit_transform(X_train_scl)\n",
    "X_test_pf_scl = pf.transform(X_test_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(): \n",
      "Train RMSE: 1.747 - Test RMSE: 1.778\n",
      "Ridge(): \n",
      "Train RMSE: 0.931 - Test RMSE: 1.005\n"
     ]
    }
   ],
   "source": [
    "linear_models = [lasso, ridge]\n",
    "for model in linear_models:\n",
    "    model.fit(X_train_pf_scl, y_train)\n",
    "    print(f'{model}: \\nTrain RMSE:',f'{rmse(X_train_pf_scl, y_train, model):.3f}', \n",
    "                    '- Test RMSE:', f'{rmse(X_test_pf_scl, y_test, model):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice how the *Lasso* does not improve (it may require fine tuning) while the *Ridge* does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "lasso_param = {'alpha':np.arange(1, 100)}\n",
    "# Ridge\n",
    "ridge_param = {'alpha':np.arange(1, 100)}\n",
    "# Decision tree\n",
    "dt_param = {'max_depth':np.arange(1, 20, 4), \n",
    "            'min_samples_split':np.arange(2, 8, 2),\n",
    "            'min_samples_leaf':np.arange(1, 20, 4)}\n",
    "\n",
    "# MLP - we can specify its architecture by defining the number of neurons at each layer \n",
    "mlp_param = {'hidden_layer_sizes': [(100,), (100, 100)],\n",
    "             'max_iter': [1000]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a nested gridsearch for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(): \n",
      "Train RMSE: 1.7336111957862568 - Best parameters: {'alpha': 1}\n",
      "Ridge(): \n",
      "Train RMSE: 0.9910981527865677 - Best parameters: {'alpha': 1}\n",
      "DecisionTreeRegressor(): \n",
      "Train RMSE: 1.2717257524102006 - Best parameters: {'max_depth': 9, 'min_samples_leaf': 5, 'min_samples_split': 6}\n",
      "MLPRegressor(max_iter=1000): \n",
      "Train RMSE: 0.9039234195101878 - Best parameters: {'hidden_layer_sizes': (100, 100), 'max_iter': 1000}\n"
     ]
    }
   ],
   "source": [
    "scoring = {'MSE': 'neg_root_mean_squared_error'}\n",
    "# store the models parameters \n",
    "models_parameters = [lasso_param, ridge_param, dt_param, mlp_param]\n",
    "\n",
    "# loop through each model, and their paramaters to estimate, and run a grid search for each of them \n",
    "for m in range(len(models)):\n",
    "    grid = GridSearchCV(models[m], models_parameters[m], scoring = scoring, refit = False, cv = 5)\n",
    "    # fit the lasso and ridge models to the polynomial scaled features \n",
    "    if m <= 1:\n",
    "        grid.fit(X_train_pf_scl, y_train)\n",
    "    # fit the deciosion tree and the MLP to the original scaled features \n",
    "    else:\n",
    "        grid.fit(X_train_scl, y_train)\n",
    "    # extract the best model \n",
    "    model_df = pd.DataFrame(grid.cv_results_).sort_values(by = 'rank_test_MSE').head(1)\n",
    "    print(f'{models[m]}: \\nTrain RMSE: {-model_df.mean_test_MSE.values[0]} - Best parameters: {model_df.params.values[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now re-train the above models with the above hyperparameters and compare them to the untuned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UNTUNED MODELS === \n",
      "\n",
      "Lasso(): \n",
      "Train RMSE: 1.752 - Test RMSE: 1.778\n",
      "Ridge(): \n",
      "Train RMSE: 1.249 - Test RMSE: 1.246\n",
      "DecisionTreeRegressor(): \n",
      "Train RMSE: 0.000 - Test RMSE: 1.345\n",
      "MLPRegressor(max_iter=1000): \n",
      "Train RMSE: 0.694 - Test RMSE: 0.876\n",
      "\n",
      "=== TUNED MODELS === \n",
      "\n",
      "Lasso(alpha=1): \n",
      "Train RMSE: 1.747 - Test RMSE: 1.778\n",
      "Ridge(alpha=1): \n",
      "Train RMSE: 0.931 - Test RMSE: 1.005\n",
      "DecisionTreeRegressor(max_depth=13, min_samples_leaf=5, min_samples_split=4): \n",
      "Train RMSE: 0.716 - Test RMSE: 1.229\n",
      "MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=1000): \n",
      "Train RMSE: 0.578 - Test RMSE: 0.857\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso()\n",
    "# Ridge\n",
    "ridge = Ridge()\n",
    "# Decision tree\n",
    "dt = DecisionTreeRegressor()\n",
    "# MLP\n",
    "mlp = MLPRegressor(max_iter = 1000)\n",
    "# store the models in a list of models \n",
    "models = [lasso, ridge, dt, mlp]\n",
    "# loop through the above list to fit, train and test the models\n",
    "print('=== UNTUNED MODELS === \\n')\n",
    "for model in models:\n",
    "    model.fit(X_train_scl, y_train)\n",
    "    print(f'{model}: \\nTrain RMSE:',f'{rmse(X_train_scl, y_train, model):.3f}', \n",
    "                    '- Test RMSE:', f'{rmse(X_test_scl, y_test, model):.3f}')\n",
    "\n",
    "lasso = Lasso(alpha = 1)\n",
    "ridge = Ridge(alpha = 1)\n",
    "dt = DecisionTreeRegressor(max_depth = 9, min_samples_leaf = 5, min_samples_split = 6)\n",
    "mlp = MLPRegressor(hidden_layer_sizes = (100, 100), max_iter = 1000)\n",
    "\n",
    "models = [lasso, ridge, dt, mlp]\n",
    "print('\\n=== TUNED MODELS === \\n')\n",
    "for m in range(len(models)):\n",
    "    if m <= 1:\n",
    "        models[m].fit(X_train_pf_scl, y_train)\n",
    "        print(f'{models[m]}: \\nTrain RMSE:',f'{rmse(X_train_pf_scl, y_train, models[m]):.3f}', \n",
    "                    '- Test RMSE:', f'{rmse(X_test_pf_scl, y_test, models[m]):.3f}')\n",
    "    # fit the deciosion tree and the MLP to the original scaled features \n",
    "    else:\n",
    "        models[m].fit(X_train_scl, y_train)\n",
    "        print(f'{models[m]}: \\nTrain RMSE:',f'{rmse(X_train_scl, y_train, models[m]):.3f}', \n",
    "                    '- Test RMSE:', f'{rmse(X_test_scl, y_test, models[m]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an improvement in performance in the tuned models and the multy-layer perceptron confirms to be the best performer among the trained models.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fdb129d5228a765a304b8d5599f04ed0b9473866110291e13ce84f812c5eadab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
