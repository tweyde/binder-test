{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2e788b",
   "metadata": {},
   "source": [
    "# Chapter 19 - Classification Models and Evaluation\n",
    "\n",
    "So far, we have only addressed machine learning with classification tasks, i.e. where there is a finite discrete set of target labels, the classes. The other main task in supervised machine learning is regression, where the task is to predict a continuous value. Regression leads to different metrics and algorithms, as well as some new concepts such as loss functions and regularization terms, as it is closer to optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1c889",
   "metadata": {},
   "source": [
    "## Classification and Probabilities\n",
    "\n",
    "A regression task is one, where the data labels are continuous numbers. In this chapter we will use the diabetes dataset, XXXXXXX "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15725af",
   "metadata": {},
   "source": [
    "### Cross-Entropy\n",
    "\n",
    "We are now going the use the iris dataset again, as in the previous chapter. We will, however, do two things differently: 1) we will **scale** the feature values and 2) we will divide the data into 2 subsets, the **training, validation and testing set**.\n",
    "\n",
    "**Scaling:** since we don't make assumptions about the data, it could be that the data values are not suitable  for the machine learning algorithms we apply. E.g. this might cause an error or the optimizer might not converge, so that we don't get a meaningful result. \n",
    "There are different scaling requirements for different models. E.g. decision trees and knn classifiers usually need to scaling. Many other models, like logistic regression or neural networks, as sensitive to scaling.   Standardization, like in chapter 8, to a mean of 0 and a standard deviation of 1 works well for many machine learning. An alternative approach is normalization, which ensures that all values are between 0 and 1. \n",
    "\n",
    "In the code below, we first split the training set from the rest and split the rest into test and validation set. Then we use the Scikit-Learn StandardScaler to standardize the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c15ffae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "dataset = load_diabetes()\n",
    "\n",
    "seed = 0 # this is used with the train_test_split to avoid random behaviour for this demo\n",
    "X = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0433b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265 89 88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[147.22471910112358, 66.47255965222679]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=.4,random_state=seed)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=.5,random_state=seed)\n",
    "print(len(y_train),len(y_val),len(y_test))\n",
    "\n",
    "# most classifiers work better with scaled input data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sclr = StandardScaler()\n",
    "sclr.fit(X_train) # scale to 0 mean and std dev 1 on training data\n",
    "\n",
    "X_train_scl = sclr.fit_transform(X_train) # scale all 3 sets:\n",
    "X_val_scl = sclr.fit_transform(X_val)\n",
    "X_test_scl = sclr.fit_transform(X_test)\n",
    "[np.mean(y_val),np.std(y_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62893f24",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "We now use another classifier: the K-Nearest--Neighbor (KNN) classifier. This classifier is very simple: is views the feature values of an item as a vectors. For a new feature vector, we calculate the $k$ closest feature vectors in our training set. We then look up the classes belonging to these $k$ feature vectors and choose the most frequent one as our KNN prediction. The number $k$ determines the behaviour. In the simplest case we can choose $1$, which makes it easy to select the predicted class (no ties).  \n",
    "\n",
    "This is a very simple classifier, but it can be quite effective. In order to select whether to use KNN or Decision Trees, we train both types of model on our training set and calculate the performance on the validation set. We also calculate the performance on the test set, which is a more realistic estimate of the performance on unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8066e80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression:  {'train': 51.78004290628544, 'val': 55.175429959614796, 'test': 59.00716319829052}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# train both linar model \n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scl,y_train)\n",
    "\n",
    "# helper functions to calculate the accuracy values on train, validation and test set. \n",
    "def rmse(X,y,predictor):\n",
    "    return np.mean(np.square(predictor.predict(X)-y))\n",
    "\n",
    "def trainValTestMse(predictor):\n",
    "    vals = {}\n",
    "    vals['train'] = rmse(X_train_scl,y_train,predictor)\n",
    "    vals['val'] = rmse(X_val_scl,y_val,predictor)\n",
    "    vals['test'] = rmse(X_test_scl,y_test,predictor)\n",
    "    return vals\n",
    "\n",
    "print(\"Linear regression: \",trainValTestMse(lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc0d6c",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5777efeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT regression:  {'train': 53.01262533452922, 'val': 61.210493352374485, 'test': 60.993757651463355}\n",
      "RF regression:  {'train': 21.49695663190402, 'val': 57.89668097306295, 'test': 58.9224391076755}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# train both model \n",
    "dtr = DecisionTreeRegressor(max_depth=3)\n",
    "dtr.fit(X_train_scl,y_train)\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=200)\n",
    "rfr.fit(X_train_scl,y_train)\n",
    "\n",
    "print(\"DT regression: \",trainValTestMse(dtr))\n",
    "print(\"RF regression: \",trainValTestMse(rfr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70580927",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Machine Learning approaches the problems of learning form data in a direct way, often relaxing the traetment of probabilities and distributions. \n",
    "The general setting is to learn a model that estimates, in ML terminology: predicts, an unknown dependent value, in ML terminology: the label. \n",
    "\n",
    "The most common form of machine learning is supervised learning, where we have labelled data that we use to fit (train) the model. \n",
    "In order to evaluate the quality of our model, we need to use a "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1808b71",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "074b26097337835a5e1dbb01309c19b1c94bf7c2401f1c92e7358cfe96e9b386"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
